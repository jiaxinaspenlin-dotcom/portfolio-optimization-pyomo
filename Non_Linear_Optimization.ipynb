{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBmvx8r88StC"
      },
      "source": [
        "#Non Linear Optimization - Group 5\n",
        "By: Jiaxin Lin, Alexandra Oteana, Daniil Rusanyuk, Juan Camilo Velasco, Michel Ward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0OfaUtcRF66"
      },
      "source": [
        "This project uses nonlinear optimization to study and improve the stock portfolios of two political figures, Josh Gottheimer and Scott Franklin. We analyze their top 10 stocks to build a risk-return model and then apply optimization techniques to find their best trading strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tnemIVFsMQE"
      },
      "source": [
        "# Josh Gottheimer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNwFkx_RlZvT"
      },
      "source": [
        "Here’s a more concise description of each stock:\n",
        "\n",
        "1. Microsoft (MSFT)\n",
        "A leading tech company known for software (Windows, Office), cloud services (Azure), and gaming (Xbox). Strong growth driven by cloud and subscription products.\n",
        "\n",
        "2. Apple (AAPL)\n",
        "Famous for its consumer electronics (iPhone, iPad), services (iCloud, Apple Music), and innovation. One of the most valuable companies globally.\n",
        "\n",
        "3. Meta (META)\n",
        "Owner of Facebook, Instagram, WhatsApp, and Oculus. Focuses on social media, digital ads, and the metaverse.\n",
        "\n",
        "4. Amazon (AMZN)\n",
        "Global leader in e-commerce and cloud computing (AWS). Also invests in entertainment, logistics, and AI.\n",
        "\n",
        "5. Johnson & Johnson (JNJ)\n",
        "A healthcare giant that makes medical devices, pharmaceuticals, and consumer health products. Known for stability and dividend payouts.\n",
        "\n",
        "6. Goldman Sachs (GS)\n",
        "A leading investment bank offering financial services like asset management, trading, and mergers & acquisitions.\n",
        "\n",
        "7. United Airlines (UAL)\n",
        "One of the largest U.S. airlines, offering domestic and international flights. Stock sensitive to economic conditions and fuel prices.\n",
        "\n",
        "8. Tesla (TSLA)\n",
        "Pioneering electric vehicle maker, also focused on renewable energy and autonomous driving technologies.\n",
        "\n",
        "9. Mastercard (MA)\n",
        "Global leader in digital payments and financial technologies, facilitating secure electronic transactions.\n",
        "\n",
        "10. Alphabet (GOOG)\n",
        "Parent company of Google, YouTube, and Waymo. Dominates online search and digital advertising, while investing in AI and autonomous driving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4LBU5zswUCl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display # Helps to display\n",
        "import random\n",
        "import yfinance as yf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from math import sqrt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMbs9B-7RXBO"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK2A8AaISQ5b"
      },
      "source": [
        "Here we are importing the dataset for Josh Gottheimer through excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcshewul6QA0",
        "outputId": "e70c6cc6-974e-4f0c-e5de-b5a132f5721a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "colab_path = \"/content/Josh Gottheimer Final.xlsx\"\n",
        "xls = pd.ExcelFile(excel_path)\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for sheet_name in xls.sheet_names:\n",
        "    # Parse the sheet and strip extra whitespace from column names\n",
        "    df = xls.parse(sheet_name)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Convert the Date column to datetime and set it as the index\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], infer_datetime_format=True, errors=\"coerce\")\n",
        "    df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "    # Use the sheet name as the ticker identifier\n",
        "    ticker = sheet_name.strip()\n",
        "\n",
        "    # Define the ticker-specific column names (these should match exactly your Excel headers)\n",
        "    open_col      = f\"Open_{ticker}\"\n",
        "    high_col      = f\"High_{ticker}\"\n",
        "    low_col       = f\"Low_{ticker}\"\n",
        "    close_col     = f\"Close_{ticker}\"\n",
        "    adj_close_col = f\"Adj Close_{ticker}\"\n",
        "    volume_col    = f\"Volume_{ticker}\"\n",
        "    buy_col       = f\"Buy_{ticker}\"\n",
        "    sell_col      = f\"Sell_{ticker}\"\n",
        "    net_col       = f\"Holding_{ticker}\"\n",
        "\n",
        "    # Clean numeric columns: remove commas and convert to float for Buy, Sell, and Net columns\n",
        "    for col in [buy_col, sell_col, net_col]:\n",
        "        if col in df.columns:\n",
        "            df.loc[:, col] = pd.to_numeric(\n",
        "                df[col].replace({',': ''}, regex=True), errors=\"coerce\"\n",
        "            ).fillna(0)\n",
        "\n",
        "    # --- Calculate Trade Metrics for this ticker ---\n",
        "    # Create a trade action column specific for this ticker\n",
        "    trade_action_col = f\"Trade_Action_{ticker}\"\n",
        "    def determine_trade_action(row):\n",
        "        # If both Buy and Sell are > 0, return \"Buy & Sell\"\n",
        "        if row[buy_col] > 0 and row[sell_col] > 0:\n",
        "            return \"Buy & Sell\"\n",
        "        elif row[buy_col] > 0:\n",
        "            return \"Buy\"\n",
        "        elif row[sell_col] > 0:\n",
        "            return \"Sell\"\n",
        "        else:\n",
        "            return \"No Trade\"\n",
        "    df[trade_action_col] = df.apply(determine_trade_action, axis=1)\n",
        "\n",
        "    # Calculate next-day return based on the ticker-specific Close column\n",
        "    next_day_return_col = f\"Next_Day_Return_{ticker}\"\n",
        "    df[next_day_return_col] = df[close_col].pct_change().shift(-1)\n",
        "\n",
        "    # Evaluate trade impact for this ticker\n",
        "    trade_impact_col = f\"Trade_Impact_{ticker}\"\n",
        "    def evaluate_trade(row):\n",
        "        action = row[trade_action_col]\n",
        "        ret = row[next_day_return_col]\n",
        "        if pd.isnull(ret):\n",
        "            return \"No Data\"\n",
        "        if action == \"Buy\":\n",
        "            return \"Good Trade\" if ret > 0 else \"Bad Trade\"\n",
        "        elif action == \"Sell\":\n",
        "            return \"Good Trade\" if ret < 0 else \"Bad Trade\"\n",
        "        elif action == \"Buy & Sell\":\n",
        "            return \"No Difference\"\n",
        "        else:\n",
        "            return \"No Trade\"\n",
        "    df[trade_impact_col] = df.apply(evaluate_trade, axis=1)\n",
        "\n",
        "    # Calculate net change in the net holding (Buy Sell) for this ticker\n",
        "    net_change_col = f\"Net_Change_{ticker}\"\n",
        "    df[net_change_col] = df[net_col].shift(1) - df[net_col]\n",
        "\n",
        "    # NEW: Relate portfolio holdings to next-day return.\n",
        "    # Multiply the net holding (Buy Sell) by the next-day return to get the dollar impact.\n",
        "    portfolio_impact_col = f\"Portfolio_Impact_{ticker}\"\n",
        "    df[portfolio_impact_col] = df[net_col] * df[next_day_return_col]\n",
        "\n",
        "    # Append the processed DataFrame to our list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all processed sheets side by side (wide) on the Date index\n",
        "df_jg = pd.concat(df_list, axis=1, join=\"outer\")\n",
        "df_jg.sort_index(inplace=True)\n",
        "\n",
        "# Display final column list and a preview of the wide DataFrame\n",
        "print(\"Final columns:\")\n",
        "print(df_jg.columns.tolist())\n",
        "print(df_jg.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "qvAHp60a6ctt",
        "outputId": "d4130c03-0992-429f-f345-b7458a884450"
      },
      "outputs": [],
      "source": [
        "df_jg.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUXIbyvgcrF"
      },
      "source": [
        "Metrics:\n",
        "\n",
        "Date, Open, High, Low, Close, Adj Close, Volume were imported from Yahoo Finance starting from 01/02/2024 to 02/24/2025.\n",
        "\n",
        "Added Metrics:\n",
        "\n",
        "1. Buy = Did they buy on that day?\n",
        "\n",
        "2. Sell = Did they sell on that day?\n",
        "\n",
        "  - Imported their buy and sell trade data from Quiver Quantitative.\n",
        "\n",
        "  - We were given ranges of how much they traded and decided used the midpoint number to make sure that it would be the most accurate in this case.\n",
        "\n",
        "3. Holding = How much do they hold in that certain stock on that certain day?\n",
        "\n",
        "  - Imported how much they money held in each stock from Quiver Quantitative\n",
        "\n",
        "4. Trade Action = Was the trade a buy or sell? Or no trade?\n",
        "\n",
        "5. Next Day Return =  Calculates the net day return based on the closed price of that stock from the previous day\n",
        "\n",
        "6. Trade Impact = If they made a trade, was it “Good” or “Bad” the next day? “No Trade” --> not evaluated\n",
        "\n",
        "7. Net Change = Calculates the net change of their hodldings in that stock from the previous day to the current day\n",
        "\n",
        "8. Portfolio Impact = How much did their overall holding's value change because of price movement, whether you traded or not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEpQ4I8MIhi5"
      },
      "source": [
        "These metrics let us see the whole picture of how a portfolio is performing. They show us how much of the change comes from the market itself and how much comes from the investor's own trading decisions. This helps us understand the underlying reasons for gains or losses in the portfolio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJS1xyy8fVk0"
      },
      "source": [
        "### Shape of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PylttGdkfTCr",
        "outputId": "4593c9c3-36dd-447e-94dd-b6cbb175a428"
      },
      "outputs": [],
      "source": [
        "df_jg.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfSWYIRxmrX"
      },
      "source": [
        "This dataset has 287 rows and 140 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv2R74KoRcs_"
      },
      "source": [
        "### .info()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqWXrlvHQuVK",
        "outputId": "7cc56e7b-e87c-4f86-ef8c-80108b959dee"
      },
      "outputs": [],
      "source": [
        "df_jg.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVV3QZ3DSLP_"
      },
      "source": [
        "- The dataframe has 287 rows (indexed by dates from 2024-01-02 to 2025-02-24)\n",
        "- There are 140 columns, with names running from “Open_MSFT” through “Portfolio_Impact_GOOG.”\n",
        "- Of the 140 columns, 102 are floating-point, 18 are integers, and 20 are objects (often strings or mixed data).\n",
        "- The entire DataFrame is using about 316 KB of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST-hNPHYu8T"
      },
      "source": [
        "### Data Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TosR9nfdWutN",
        "outputId": "71e5a02e-dca2-4a1b-b894-364eecda3e2a"
      },
      "outputs": [],
      "source": [
        "# Increase the max rows (or columns) displayed in the console\n",
        "pd.set_option('display.max_rows', None)  # No limit on rows\n",
        "# pd.set_option('display.max_columns', None)  # No limit on columns if needed\n",
        "\n",
        "# Now printing dtypes will not truncate\n",
        "print(\"Data types for df_jg columns:\")\n",
        "print(df_jg.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrBQKEBMY-Kl"
      },
      "source": [
        "Checking if all the data types are accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7875chhGYOdw"
      },
      "outputs": [],
      "source": [
        "for col in df_jg.columns:\n",
        "    if \"Trade_Action\" in col:\n",
        "        df_jg[col] = df_jg[col].astype('category')\n",
        "    elif \"Trade_Impact\" in col:\n",
        "        df_jg[col] = df_jg[col].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO7hz1cJZFJ7"
      },
      "source": [
        "Changing the data type for Trade_Action_{ticker} and Trade_Impact{ticker) to category instead of object to help with analysis and uses less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT9yKeJRYeUz",
        "outputId": "56bc6594-1971-4d9d-c3fc-d80d153a9488"
      },
      "outputs": [],
      "source": [
        "# Increase the max rows (or columns) displayed in the console\n",
        "pd.set_option('display.max_rows', None)  # No limit on rows\n",
        "# pd.set_option('display.max_columns', None)  # No limit on columns if needed\n",
        "\n",
        "# Now printing dtypes will not truncate\n",
        "print(\"Data types for df_jg columns:\")\n",
        "print(df_jg.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZWlO1L_Zx40"
      },
      "source": [
        "Checking to see if the changes were made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQAUddTARStZ"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8bimVQF7Q8Rt",
        "outputId": "353c0fc1-0d46-469d-b0f7-0eb09d111d50"
      },
      "outputs": [],
      "source": [
        "df_jg.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "60TtahpGRDu_",
        "outputId": "923254ca-b0f0-4f23-d5a6-d94d22692551"
      },
      "outputs": [],
      "source": [
        "# Drop rows with any missing values\n",
        "df_jg.dropna(inplace=True)\n",
        "\n",
        "# Display the DataFrame after removing missing values\n",
        "print(df_jg.head(10))\n",
        "df_jg.info()\n",
        "df_jg.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf5V3CgafkoJ",
        "outputId": "eaf8cfbc-3355-4296-e54a-9c2d59326d87"
      },
      "outputs": [],
      "source": [
        "df_jg.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI99jk_gSMhX"
      },
      "source": [
        "Checking for missing values. The missing values are coming from Next_Day_Return_{ticker} which calculates the net day return based on the closed price of that stock from the previous day and since the data starts from 2024-01-02 and there is no data on 2024-01-01 therefore it shows up NaN so we are deleting them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkXOyTj8moC"
      },
      "source": [
        "##Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd1qpEh-8r84"
      },
      "source": [
        "###Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPxPGeH0xaxV"
      },
      "source": [
        "#### MSFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "QlL0-9SqxU5W",
        "outputId": "eebb6555-e440-41a9-842c-a2737b95320f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MSFT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "msft_columns = [f\"{feature}_MSFT\" for feature in features]\n",
        "\n",
        "# Filter dataset for MSFT, dropping rows with NaNs\n",
        "df_msft = df_jg[msft_columns].dropna()\n",
        "\n",
        "# Set up color palette for MSFT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MSFT\"\n",
        "    if col_name in df_msft.columns:\n",
        "        sns.histplot(data=df_msft, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of MSFT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_MSFT\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi5AyxvCYXL"
      },
      "source": [
        "For MSFT, we can see that all of the price based features and returns have bell shaped curves showing that during this specific time period, there were no harsh movements in Microsoft’s stock price. In addition, in regards to Volume, Buy and Sell activity, all of their histograms are right skewed which could mean that across this time period, Microsoft’s stock had a relatively low trading activity but there were some specific days that had a very high trading activity. After performing some research we came up with a very interesting analysis and is that across the year we saw that Microsoft had a very low volatility with small earnings and losses but there are some days in the time period that there were high volume of trading normally investors that have a notable position decide to either sell or buy the stock."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDw66Ipoxc9x"
      },
      "source": [
        "#### JNJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "KwTAi5kRxcr_",
        "outputId": "39b6ba66-5b7a-4c8c-ce6c-3e1909743d73"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JNJ features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jnj_columns = [f\"{feature}_JNJ\" for feature in features]\n",
        "\n",
        "# Filter dataset for JNJ, dropping rows with NaNs\n",
        "df_jnj = df_jg[jnj_columns].dropna()\n",
        "\n",
        "# Set up color palette for JNJ\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JNJ\"\n",
        "    if col_name in df_jnj.columns:\n",
        "        sns.histplot(data=df_jnj, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of JNJ {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_JNJ\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-AX8qC6uV0D"
      },
      "source": [
        "For JNJ, the price distributions are multimodal which means that there were notable changes in the stock price through this time period. In addition, the High and Sell activity is based on just a few large transactions and that is why we believe these histograms have a right skewed distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCaB296Kx27i"
      },
      "source": [
        "#### GOOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "CZmGJ6E2xrH6",
        "outputId": "b0a0b2a1-26cf-4c3e-dbb1-4eca9c61b9a4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GOOG features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "goog_columns = [f\"{feature}_GOOG\" for feature in features]\n",
        "\n",
        "# Filter dataset for GOOG, dropping rows with NaNs\n",
        "df_goog = df_jg[goog_columns].dropna()\n",
        "\n",
        "# Set up color palette for GOOG\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GOOG\"\n",
        "    if col_name in df_goog.columns:\n",
        "        sns.histplot(data=df_goog, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of GOOG {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_GOOG\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFlcKyM6uYwX"
      },
      "source": [
        "For the price based features for GOOG, we can see that the histograms have a normal distribution where the price of the stock ranges between 140 and 200. These histograms show us that across this time period, Google’s stock price maintained very stable. Once again, just like in the previous stocks, we can see that based on these histograms, most of the days, this stock had a low trading volume, but there were some days in which important spikes occurred. In addition, from these histograms we can also infer that although there were a low amount of trades, there are a few massive trades which likely were made by institutional investors. Moreover, the Next-Day returns also follow a normal distribution which means that daily returns are normally very stable and the closing price of the GOOG stock tends to be very similar to the opening price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mS9qvOjx4hg"
      },
      "source": [
        "#### AAPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "rNcLo71Wx6dg",
        "outputId": "d70635aa-7b2e-4d76-9d05-a3cd00fe0c73"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AAPL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "aapl_columns = [f\"{feature}_AAPL\" for feature in features]\n",
        "\n",
        "# Filter dataset for AAPL, dropping rows with NaNs\n",
        "df_aapl = df_jg[aapl_columns].dropna()\n",
        "\n",
        "# Set up color palette for AAPL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AAPL\"\n",
        "    if col_name in df_aapl.columns:\n",
        "        sns.histplot(data=df_aapl, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of AAPL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_AAPL\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5YDZUFKub_6"
      },
      "source": [
        "For the price based features for Apple’s stock, we can see that the histograms have multimodal or bi-modal distributions which means that the prices tend to be around a larger range (180-260). On the other hand, the volume distribution is right skewed which means that there are a few days in the year that have very high trading activity. Last but not least important, in regards to return, the histograms have a normal distribution meaning that daily returns are balanced among gains and losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxWTYg-Mx6xg"
      },
      "source": [
        "#### GS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "-15oMNKIx8Xa",
        "outputId": "e811edbd-5b3b-46d0-ab3a-c5c1a168016e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GS features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "gs_columns = [f\"{feature}_GS\" for feature in features]\n",
        "\n",
        "# Filter dataset for GS, dropping rows with NaNs\n",
        "df_gs = df_jg[gs_columns].dropna()\n",
        "\n",
        "# Set up color palette for GS\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GS\"\n",
        "    if col_name in df_gs.columns:\n",
        "        sns.histplot(data=df_gs, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of GS {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_GS\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfzXpCTkufNX"
      },
      "source": [
        "Once again, the GS stock has a bi-modal or multimodal distribution in the price based features which means that this stock had one of the largest price ranges during this time period (400 and 650). In other words, this means that this stock fluctuated way more than the previous stocks. In addition, just like the previous stocks, there were some days in which the trading volume was way higher than the rest of the days. Moreover, the next day's return histogram which has a normal distribution helps us conclude that most days of the year, this stock did not experience harsh changes but there were some days that the stock had notable and very high peaks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmAko6xax87Q"
      },
      "source": [
        "#### TSLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "1Lwwyeb_x-cf",
        "outputId": "826b9604-1f57-4187-8c8d-d4e6b9b5accc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant TSLA features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "tsla_columns = [f\"{feature}_TSLA\" for feature in features]\n",
        "\n",
        "# Filter dataset for TSLA, dropping rows with NaNs\n",
        "df_tsla = df_jg[tsla_columns].dropna()\n",
        "\n",
        "# Set up color palette for TSLA\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_TSLA\"\n",
        "    if col_name in df_tsla.columns:\n",
        "        sns.histplot(data=df_tsla, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of TSLA {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_TSLA\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifY3iIThuh94"
      },
      "source": [
        "In these histograms there are some important conclusions that we were able to make. First, the Tesla price distributions are highly skewed with a large range of prices from 200 up to over 450. Furthermore, TSLA is the first stock that we see that actually has a moderate trading volume all across the year but once again, there are some specific days in which the trading volume spikes. Also, something different about this stock is that Buy and Sell distributions are really similar and very concentrated near 0 which means that most trades made in this stock are small trades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPFihDOsx-sO"
      },
      "source": [
        "#### MA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "1ieJW2wxx_WN",
        "outputId": "6e9f3a95-2491-4fbc-c4e6-6d64092b4939"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MA features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "ma_columns = [f\"{feature}_MA\" for feature in features]\n",
        "\n",
        "# Filter dataset for MA, dropping rows with NaNs\n",
        "df_ma = df_jg[ma_columns].dropna()\n",
        "\n",
        "# Set up color palette for MA\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MA\"\n",
        "    if col_name in df_ma.columns:\n",
        "        sns.histplot(data=df_ma, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of MA {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_MA\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbILGEypukk0"
      },
      "source": [
        "For MasterCard, we can see that the price features follow a somewhat normal distribution with some multimodal characteristics. Also, this stock has a price range between 425 and 575 which when compared to the other stocks that he has seen before in the project, it shows that this stock has a low volatility. Furthermore, we can see that in the Buy and Sell distributions, the histograms are deeply concentrated near 0 which means that most of the stock transactions are small. Lastly, in the Next Day Return histogram, we can see a normal distribution concentrated in 0 which translates to that most days the stock will close at a similar price ta]han the closing price of the day before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZF-qwmWyBrt"
      },
      "source": [
        "#### UAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "GZbZCZIJyC_3",
        "outputId": "1f1930f4-edb2-4c5c-ac84-8cc4ae86cea6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant UAL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "ual_columns = [f\"{feature}_UAL\" for feature in features]\n",
        "\n",
        "# Filter dataset for UAL, dropping rows with NaNs\n",
        "df_ual = df_jg[ual_columns].dropna()\n",
        "\n",
        "# Set up color palette for UAL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_UAL\"\n",
        "    if col_name in df_ual.columns:\n",
        "        sns.histplot(data=df_ual, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of UAL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_UAL\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMxKc9EBuoz7"
      },
      "source": [
        "The United Airlines price feature histograms are also heavily skewed and the long tail in the right means that across this time period stays at lower prices but there were some days that had sharp price increases. In regards to trading volume, the histogram is highly skewed allowing us to conclude that there are some days that this stock had extreme trading spikes. On the other hand, we could conclude from the Buy, Sell and Holding histograms that Senator Gottheimer did not trade this stock during this specific time period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9nFSJi9yDWl"
      },
      "source": [
        "#### AMZN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "dqbKWpJzyEuv",
        "outputId": "f34da34f-a5f4-4a56-9e06-9c7c87885820"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AMZN features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "amzn_columns = [f\"{feature}_AMZN\" for feature in features]\n",
        "\n",
        "# Filter dataset for AMZN, dropping rows with NaNs\n",
        "df_amzn = df_jg[amzn_columns].dropna()\n",
        "\n",
        "# Set up color palette for AMZN\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AMZN\"\n",
        "    if col_name in df_amzn.columns:\n",
        "        sns.histplot(data=df_amzn, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of AMZN {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_AMZN\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF-6K85_uuDo"
      },
      "source": [
        "In the price feature histograms of this stock we can witness that they have a near normal distribution with a small right skew. In addition, the flat distributions that we have in the Buy, Sell and Holding histograms mean that Senator Gottheimer did not trade this stock during this specific time period. Also, if we take a look at the Return histogram, we can conclude that based on its normal distribution Amazon’s returns are quite stable making this stock a low risk investment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxLcZB2lyE7I"
      },
      "source": [
        "#### META"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "ogXoa07XyGWI",
        "outputId": "a7a29fa6-8faa-4ac4-fd6f-a8df02b7de68"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant META features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "meta_columns = [f\"{feature}_META\" for feature in features]\n",
        "\n",
        "# Filter dataset for META, dropping rows with NaNs\n",
        "df_meta = df_jg[meta_columns].dropna()\n",
        "\n",
        "# Set up color palette for META\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_META\"\n",
        "    if col_name in df_meta.columns:\n",
        "        sns.histplot(data=df_meta, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of META {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_META\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuJZEaVSyR4q"
      },
      "source": [
        "In this case, Meta’s stock prices are slightly right skewed which means that higher price movements were more frequent than large declines. Meta was another stock that fluctuated notably being in a range from 400 to 700, but most of the time was between 500 and 600. Once again, we have some flat distributions in some of the histograms which tells us that Senator Gottheimer did not buy any of this stock during this time period. Based on the Return histograms, we can see that Meta has stable return patterns making this stock a low risk investment opportunity for the Senator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EgOYEEH8vby"
      },
      "source": [
        "###Boxplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NkiXm1Z9DxU"
      },
      "source": [
        "####MSFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "yyNmCOPv8vBO",
        "outputId": "542822bd-45df-4ac9-cdad-f3f1b5c82f9f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MSFT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "msft_columns = [f\"{feature}_MSFT\" for feature in features]\n",
        "\n",
        "# Filter dataset for MSFT\n",
        "df_msft = df_jg[msft_columns].dropna()\n",
        "\n",
        "# Set up color palette for MSFT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MSFT\"\n",
        "    if col_name in df_msft.columns:\n",
        "        sns.boxplot(y=df_msft[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"MSFT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLm4QVZO2HlN"
      },
      "source": [
        "These boxplots for MSFT show a stable price range across Open, High, Low, Close, and Adjusted Close values. The volume chart has some notable outliers, which means that there were some occasional spikes in trading activity. The Next-Day Return appears to be well distributed with some outliers, indicating some high daily movements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-tb9uHg9d3p"
      },
      "source": [
        "####JNJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "vNank0NI9i3s",
        "outputId": "71dfd7e0-18bd-408b-a0e5-3ad5d8190279"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JNJ features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jnj_columns = [f\"{feature}_JNJ\" for feature in features]\n",
        "\n",
        "# Filter dataset for JNJ\n",
        "df_jnj = df_jg[jnj_columns].dropna()\n",
        "\n",
        "# Set up color palette for JNJ\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JNJ\"\n",
        "    if col_name in df_jnj.columns:\n",
        "        sns.boxplot(y=df_jnj[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"JNJ {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwb-AVqZpbm0"
      },
      "source": [
        "JNJ’s price related features are tightly clustered with small interquartile ranges, showing lower volatility compared to the other stocks. Nevertheless, trading volume shows a notable variability, with outliers indicating some spikes. The Next-Day Return is around zero, which suggests a balanced distribution of daily price fluctuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuZyHIGg98hY"
      },
      "source": [
        "####GOOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "BlcrrEgH-QD8",
        "outputId": "be00787c-fb8e-4c4f-a1e5-78b2ba8af1d5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GOOG features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "goog_columns = [f\"{feature}_GOOG\" for feature in features]\n",
        "\n",
        "# Filter dataset for GOOG\n",
        "df_goog = df_jg[goog_columns].dropna()\n",
        "\n",
        "# Set up color palette for GOOG\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GOOG\"\n",
        "    if col_name in df_goog.columns:\n",
        "        sns.boxplot(y=df_goog[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"GOOG {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfr-rLVO3hsA"
      },
      "source": [
        "Based on these boxplots we can conclude that GOOG has a larger range in price-related boxplots which shows that this stock had a higher volatility compared to JNJ. In addition, the trading volume has notable outliers, which means that there were a few days in this time frame that had a higher trading activity. The Next-Day Return and Net Change graphs show balanced distributions but with some extreme values, demonstrating some large price swings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcGJ7U--TEn"
      },
      "source": [
        "####AAPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "mkkLtjD0-W6e",
        "outputId": "9fc2ce4f-271b-4fba-f897-548658bb4368"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AAPL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "aapl_columns = [f\"{feature}_AAPL\" for feature in features]\n",
        "\n",
        "# Filter dataset for AAPL\n",
        "df_aapl = df_jg[aapl_columns].dropna()\n",
        "\n",
        "# Set up color palette for AAPL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AAPL\"\n",
        "    if col_name in df_aapl.columns:\n",
        "        sns.boxplot(y=df_aapl[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"AAPL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7x9EIVE4RyH"
      },
      "source": [
        "AAPL’s price movements have a bigger range, with a relatively higher median price level. Volume once again has a high number of outliers, showing moments of elevated trading activity. The Next-Day Return and Net Change show a normal distribution but with some deviations, which could translate to short-term price changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sn05Spt-api"
      },
      "source": [
        "####GS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qSk-9Oi--coY",
        "outputId": "16112a19-1509-4c5a-f296-e4a0a13c4f4e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GS features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "gs_columns = [f\"{feature}_GS\" for feature in features]\n",
        "\n",
        "# Filter dataset for GS\n",
        "df_gs = df_jg[gs_columns].dropna()\n",
        "\n",
        "# Set up color palette for GS\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GS\"\n",
        "    if col_name in df_gs.columns:\n",
        "        sns.boxplot(y=df_gs[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"GS {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJD3p8c5qzM5"
      },
      "source": [
        "GS price related features distributions show steady trading patterns, having also some notable outliers in Volume and Next-Day Return, which translates once again to occasional high volatility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwelNtIY-esJ"
      },
      "source": [
        "####TSLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "nLfxrNQl-gd7",
        "outputId": "14097546-6e49-4c80-f8d8-bc00a6a3a09d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant TSLA features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "tsla_columns = [f\"{feature}_TSLA\" for feature in features]\n",
        "\n",
        "# Filter dataset for TSLA\n",
        "df_tsla = df_jg[tsla_columns].dropna()\n",
        "\n",
        "# Set up color palette for TSLA\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_TSLA\"\n",
        "    if col_name in df_tsla.columns:\n",
        "        sns.boxplot(y=df_tsla[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"TSLA {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORfEStEvI2x"
      },
      "source": [
        "Based on these histograms, Tesla’s stock has a high volatility, with a big range in its open, high, low, and close prices, counting as well with numerous outliers, indicating strong price fluctuations. The trading volume also shows high variation, suggesting some days of very active trading. On the other hand, the Holding distribution is more dispersed, indicating that some positions in TSLA were significantly large. Lastly, the Next-Day Return distribution shows wide variability with some extreme values, which allows us to conclude that  Tesla is a high risk but also high reward stock option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qedQbf-jaN"
      },
      "source": [
        "####MA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "iX-4lJQu-lGz",
        "outputId": "8e897b2b-1a5a-4567-bfe3-f7923cbb48fa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MA features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "ma_columns = [f\"{feature}_MA\" for feature in features]\n",
        "\n",
        "# Filter dataset for MA\n",
        "df_ma = df_jg[ma_columns].dropna()\n",
        "\n",
        "# Set up color palette for MA\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MA\"\n",
        "    if col_name in df_ma.columns:\n",
        "        sns.boxplot(y=df_ma[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"MA {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXKoqRlaxjKT"
      },
      "source": [
        "Based on these boxplots, Mastercard shows a low volatility in the price driving features. Also, trading volume has some occasional spikes. The Buy and Sell distributions are close to zero, indicating that most trades are small transactions rather than large institutional shifts. The Next-Day Return is centered around zero, showing a steady daily performance which makes this stock a low risk investment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gum2nURU-nV2"
      },
      "source": [
        "####UAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "TGFZoJeU-qRQ",
        "outputId": "2d4cf5c0-ff2b-4ff2-c8b2-dfb7b1502341"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant UAL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "ual_columns = [f\"{feature}_UAL\" for feature in features]\n",
        "\n",
        "# Filter dataset for UAL\n",
        "df_ual = df_jg[ual_columns].dropna()\n",
        "\n",
        "# Set up color palette for UAL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_UAL\"\n",
        "    if col_name in df_ual.columns:\n",
        "        sns.boxplot(y=df_ual[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"UAL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvZ6onnd1H1l"
      },
      "source": [
        "Based on these boxplots, UAL's stock has a significant price volatility with many outliers, indicating many notable sharp movements. Trading volume is also highly skewed, with some extreme spikes. In addition, once again the Next-day returns are mostly centered around zero, but some outliers confirm the sharp changes. Overall, UAL shows high signs of volatility making it a higher risk stock."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mERKY5VH-uqg"
      },
      "source": [
        "####AMZN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "27cuVdki-w4b",
        "outputId": "4c536bc8-ecb8-4314-c741-c28133ccd2ba"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AMZN features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "amzn_columns = [f\"{feature}_AMZN\" for feature in features]\n",
        "\n",
        "# Filter dataset for AMZN\n",
        "df_amzn = df_jg[amzn_columns].dropna()\n",
        "\n",
        "# Set up color palette for AMZN\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AMZN\"\n",
        "    if col_name in df_amzn.columns:\n",
        "        sns.boxplot(y=df_amzn[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"AMZN {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLDs1Xjp1yAL"
      },
      "source": [
        "In this case, for the AMZN stock the price-related features show relatively stable distributions with not as many  outliers, indicating a constant trading activity. The volume data on the other hand shows significant variation, with many outliers, which makes us conclude that there were some spikes in the trading activity. The next-day return distribution is centered around zero, demonstrating a balanced daily performance overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJubUiz0-zS4"
      },
      "source": [
        "####META"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ZZEqpmz_-1c7",
        "outputId": "8472846b-5e39-4410-cfdf-088e2efd7860"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant META features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "meta_columns = [f\"{feature}_META\" for feature in features]\n",
        "\n",
        "# Filter dataset for META\n",
        "df_meta = df_jg[meta_columns].dropna()\n",
        "\n",
        "# Set up color palette for META\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_META\"\n",
        "    if col_name in df_meta.columns:\n",
        "        sns.boxplot(y=df_meta[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"META {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrrVRPaz9FS"
      },
      "source": [
        "Meta’s stock had relatively stable price movements but it also had some high spikes. Trading volume had some high-activity days. Furthermore, the low Buy/Sell activity means that this stock was not traded during this time period. Moreover, returns were mostly stable, but certain days saw notable fluctuations, impacting the overall portfolio performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw0dwRDQsnF6"
      },
      "source": [
        "## Analysis of Josh Gottheimer's investment portfolio\n",
        "\n",
        "Analyzing Representative Josh Gottheimer's investment portfolio reveals a strategic emphasis on technology and large-cap companies, reflecting both his professional background and a focus on stable, long-term growth.​\n",
        "\n",
        "1. Professional Background Influence:\n",
        "\n",
        "Microsoft: Gottheimer's substantial allocation to Microsoft (MSFT) at 58.45% of his portfolio aligns with his prior role as a strategist for 3 years. This significant investment suggests strong confidence in Microsoft's performance and prospects. ​\n",
        "\n",
        "2. Portfolio Composition:\n",
        "\n",
        "- Technology Sector Dominance:\n",
        "\n",
        "Major Holdings: Beyond Microsoft, Gottheimer's investments in Apple (AAPL - 3.28%), Alphabet (GOOG - 0.28%), Tesla (TSLA - 0.29%), and Meta Platforms (META - 1.44%) underscore a robust commitment to the technology sector.​\n",
        "\n",
        "- Diversification Across Sectors:\n",
        "\n",
        "- Consumer Discretionary:\n",
        "\n",
        "Investments in Amazon (AMZN - 1.61%) and Tesla reflect exposure to e-commerce and innovative automotive industries.​\n",
        "\n",
        "Financials: Holdings in Goldman Sachs (GS - 0.33%) and Mastercard (MA - 0.28%) provide access to traditional banking and payment processing sectors.​\n",
        "\n",
        "Healthcare: A stake in Johnson & Johnson (JNJ - 0.35%) offers stability through a leading healthcare conglomerate.​\n",
        "\n",
        "Transportation: Investment in United Airlines (UAL - 0.32%) indicates exposure to the aviation industry.​\n",
        "\n",
        "3. Investment Strategy Insights:\n",
        "\n",
        "Concentration in Familiar Entities: The significant investment in Microsoft suggests a preference for companies where Gottheimer possesses in-depth knowledge, potentially enhancing investment confidence.​\n",
        "\n",
        "Emphasis on Technology: Allocating over 60% of the portfolio to tech companies indicates a strong belief in the sector's growth potential and resilience.​\n",
        "\n",
        "Selective Diversification: While technology dominates, the inclusion of companies from various sectors demonstrates a strategy to balance growth with stability, mitigating sector-specific risks.​\n",
        "\n",
        "\n",
        "In summary, Josh Gottheimer's portfolio reflects a strategic blend of leveraging personal expertise and targeting high-growth sectors, complemented by diversification to ensure long-term, stable returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJLeorRAsE_r"
      },
      "source": [
        "## Bivariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiQhNZvcsHX7"
      },
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AuDumgUJBbd0",
        "outputId": "8d89cb73-0251-41ca-e105-87a325c3db7e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract tickers from column names while excluding non-ticker suffixes\n",
        "tickers = {col.split('_')[-1] for col in df_jg.columns\n",
        "           if '_' in col and col.split('_')[-1] not in ['Shares', 'Held']}\n",
        "\n",
        "# Define relevant features[\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Number of tickers\n",
        "num_tickers = len(tickers)\n",
        "\n",
        "# Create subplots (one per ticker) with a larger size\n",
        "fig, axes = plt.subplots(num_tickers, 1, figsize=(10, num_tickers * 7), squeeze=False)\n",
        "\n",
        "# Loop through tickers to compute and plot correlation matrices\n",
        "for idx, ticker in enumerate(sorted(tickers)):  # Sort tickers alphabetically for consistency\n",
        "    # Select relevant columns for the ticker\n",
        "    ticker_cols = [f\"{feature}_{ticker}\" for feature in features if f\"{feature}_{ticker}\" in df_jg.columns]\n",
        "\n",
        "    # Extract data and drop missing values\n",
        "    ticker_data = df_jg[ticker_cols].dropna()\n",
        "\n",
        "    # Ensure there are at least two columns for correlation calculation\n",
        "    if ticker_data.shape[1] < 2:\n",
        "        print(f\"Skipping correlation matrix for {ticker} due to insufficient data.\")\n",
        "        continue\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    ticker_corr = ticker_data.corr()\n",
        "\n",
        "    # Plot heatmap\n",
        "    ax = axes[idx, 0]\n",
        "    sns.heatmap(ticker_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=ax)\n",
        "    ax.set_title(f\"Correlation Matrix for {ticker.upper()}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwn46jWLoXkC"
      },
      "source": [
        "\n",
        "\n",
        "Values range from -1 to 1:\n",
        "1.00 → Perfect positive correlation (Both variables move in the same direction).\n",
        "-1.00 → Perfect negative correlation (One goes up, the other goes down).\n",
        "0.00 → No correlation (Variables move independently).\n",
        "\n",
        "Key Observations from Correlation Matrix:\n",
        "\n",
        "1. Price-Based Features (Open, High, Low, Close, Adj Close)\n",
        "\n",
        "High correlation (~0.9 - 1.0) between Open, High, Low, Close, and Adj Close.\n",
        "This is expected since these values track the price movements and are interdependent.\n",
        "\n",
        "2. Volume vs. Price\n",
        "\n",
        "Volume may show weak or inverse correlation with Close:\n",
        "If Volume ↑ and Close ↓, it means selling pressure is driving prices down.\n",
        "If Volume ↑ and Close ↑, it suggests strong demand for the stock.\n",
        "3. Stock-Specific Insights\n",
        "\n",
        "For volatile stocks (tech stocks like AAPL, TSLA, NVDA):\n",
        "Volume may fluctuate significantly, showing lower correlation with price.\n",
        "For stable blue-chip stocks ( JNJ, PG):\n",
        "Volume and price may have a steady relationship, leading to a stronger correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRNXjWY1oXVm"
      },
      "source": [
        "1. MSFT (Microsoft)\n",
        "- Strong positive correlation: Between Open, High, Low, and Close, indicating smooth price movements.\n",
        "- Moderate correlation: Between Volume and Net_Change, suggesting price shifts occur on high-volume days.\n",
        "- Weak correlation: Between Next_Day_Return and historical prices, implying some unpredictability.\n",
        "\n",
        "2. AAPL (Apple)\n",
        "- High correlation: Between Portfolio_Impact and Net_Change, indicating stock price moves significantly affect portfolios.\n",
        "- Negative correlation: Between Buy and Next_Day_Return, suggesting traders may buy dips but short-term returns are unpredictable.\n",
        "\n",
        "3. META (Meta Platforms)\n",
        "- Strong correlation: Between Next_Day_Return and Portfolio_Impact, reflecting Meta’s volatility.\n",
        "- Weak correlation: Between Volume and Price, indicating social sentiment likely drives stock movements more than volume alone.\n",
        "\n",
        "4. AMZN (Amazon)\n",
        "- Moderate correlation: Between Volume and Net_Change, showing some connection between trading volume and price movements.\n",
        "- Low correlation: Between Buy signals and next-day returns, meaning immediate price reactions to purchases are weak.\n",
        "\n",
        "5. JNJ (Johnson & Johnson)\n",
        "- Low correlation: Across most metrics, suggesting a defensive stock with steady price action.\n",
        "- Weak Buy correlation: Indicates that buying activity does not drive strong short-term price changes.\n",
        "\n",
        "6. GS (Goldman Sachs)\n",
        "- Strong correlation: Between Volume and Net_Change, as financial stocks react quickly to trading flows.\n",
        "- Moderate inverse correlation: Between Holding and Next_Day_Return, suggesting traders adjust positions frequently.\n",
        "\n",
        "7. UAL (United Airlines)\n",
        "- High correlation: Between Volume and Price changes, reflecting travel industry volatility.\n",
        "- Strong negative correlation: Between Net_Change and Holding, suggesting traders exit positions before large moves.\n",
        "\n",
        "8. TSLA (Tesla)\n",
        "- Low correlation: Between most indicators, showing extreme volatility and unpredictability.\n",
        "- Weak correlation: Between Buy and Next_Day_Return, meaning purchases do not immediately reflect price gains.\n",
        "\n",
        "9. MA (Mastercard)\n",
        "- Moderate correlation: Between Holding and Portfolio_Impact, meaning long-term positions drive portfolio value.\n",
        "- Low correlation: Between Volume and Price, suggesting steady movement without erratic volume shifts.\n",
        "\n",
        "10. GOOG (Alphabet)\n",
        "- Strong correlation: Between Next_Day_Return and Portfolio_Impact, showing that stock moves affect holdings significantly.\n",
        "- Weak correlation: Between Volume and Price, reinforcing that Google’s price changes are not driven by short-term volume spikes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzvZJ_B_s6Ij"
      },
      "source": [
        "## Pair Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vdo_OiNOs38V",
        "outputId": "91408610-d827-4121-a567-2562799cae9f"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract tickers from column names while excluding non-ticker suffixes\n",
        "tickers = {col.split('_')[-1] for col in df_jg.columns # Changed stock_data_JG to df_jg\n",
        "           if '_' in col and col.split('_')[-1] not in ['Shares', 'Held']}\n",
        "\n",
        "# Define relevant features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Loop through tickers and generate pairplots\n",
        "for ticker in sorted(tickers):  # Sort for consistent order\n",
        "    # Select relevant columns for the ticker\n",
        "    ticker_cols = [f\"{feature}_{ticker}\" for feature in features if f\"{feature}_{ticker}\" in df_jg.columns] # Changed stock_data_JG to df_jg\n",
        "\n",
        "    # Extract data and drop missing values\n",
        "    ticker_data = df_jg[ticker_cols].dropna() # Changed stock_data_JG to df_jg\n",
        "\n",
        "    # Ensure there are at least two numeric columns\n",
        "    if ticker_data.shape[1] < 2:\n",
        "        print(f\"Skipping pairplot for {ticker} due to insufficient data.\")\n",
        "        continue\n",
        "\n",
        "    # Rename columns for better readability in pairplot\n",
        "    ticker_data = ticker_data.rename(columns={col: col.replace(f\"_{ticker}\", \"\") for col in ticker_cols})\n",
        "\n",
        "    # Generate pairplot\n",
        "    print(f\"Generating pairplot for {ticker}...\")\n",
        "    sns.pairplot(ticker_data, diag_kind=\"kde\", corner=True)  # `corner=True` removes duplicate plots\n",
        "    plt.suptitle(f\"Pairplot for {ticker.upper()}\", y=1.02)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5JFXHdpsML-"
      },
      "source": [
        "\n",
        "\n",
        "1. Price Features (Open, High, Low, Close, Adj Close)\n",
        "Highly correlated (~0.9 - 1.0), as these metrics track the same price movement.\n",
        "Diagonal KDE plots show distribution shapes (e.g., normal or skewed).\n",
        "2. Volume vs. Price\n",
        "Volume vs. Close:\n",
        "If Volume ↑ but Close ↓ → Selling pressure (large sell-offs).\n",
        "If Volume ↑ and Close ↑ → Strong demand (low offers).\n",
        "3. Detecting Outliers\n",
        "Scatterplots highlight anomalies in price or volume data.\n",
        "If a stock has sudden volume spikes, it may indicate news impact or earnings reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkZLicLiC1L8"
      },
      "source": [
        "## Hypothesis JG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyn-NstQDQer"
      },
      "source": [
        "### Does Higher Volume_GOOG (x-axis) correlate with lower Close_MSFT (y-axis) and is correlated to Close_APPL (by color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "wgxC75gPC0nA",
        "outputId": "e32be62f-d262-4a45-909f-c0878ce67821"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "# Ensure df_jg is already created using your previous code\n",
        "\n",
        "# Define relevant columns\n",
        "volume_goog = \"Volume_GOOG\"\n",
        "close_msft = \"Close_MSFT\"\n",
        "close_appl = \"Close_AAPL\"\n",
        "\n",
        "# Drop missing values to ensure a clean dataset for analysis\n",
        "df_clean = df_jg[[volume_goog, close_msft, close_appl]].dropna()\n",
        "\n",
        "# Compute correlation coefficients\n",
        "corr_volume_close_msft, _ = spearmanr(df_clean[volume_goog], df_clean[close_msft])\n",
        "corr_volume_close_appl, _ = spearmanr(df_clean[volume_goog], df_clean[close_appl])\n",
        "corr_msft_appl, _ = spearmanr(df_clean[close_msft], df_clean[close_appl])\n",
        "\n",
        "print(f\"Spearman correlation between {volume_goog} and {close_msft}: {corr_volume_close_msft:.4f}\")\n",
        "print(f\"Spearman correlation between {volume_goog} and {close_appl}: {corr_volume_close_appl:.4f}\")\n",
        "print(f\"Spearman correlation between {close_msft} and {close_appl}: {corr_msft_appl:.4f}\")\n",
        "\n",
        "# Create a scatterplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(\n",
        "    df_clean[volume_goog],\n",
        "    df_clean[close_msft],\n",
        "    c=df_clean[close_appl],\n",
        "    cmap='coolwarm',\n",
        "    edgecolor='k',\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.colorbar(label='Close_AAPL')\n",
        "plt.xlabel(\"Volume_GOOG\")\n",
        "plt.ylabel(\"Close_MSFT\")\n",
        "plt.title(\"Multivariate Scatter Plot: Volume_GOOG vs Close_MSFT (colored by Close_AAPL)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC1Ncu-MDgP0"
      },
      "source": [
        "For this hypothesis, we selected Google (GOOG), Microsoft (MSFT), and Apple (AAPL) because they are part of the \"Magnificent 7\"—a group of leading tech stocks that have significantly driven market growth in recent years. These companies are among the most influential in the stock market due to their high market capitalization, strong financial performance, and significant impact on major indices like the S&P 500 and Nasdaq.\n",
        "\n",
        "Given their dominance in the technology sector, we wanted to explore whether trading volume in GOOG correlates with price movements in MSFT, while AAPL’s closing price serves as a reference point to observe broader market trends. The hypothesis is based on the idea that trading patterns in one major tech stock could influence or be influenced by another, as institutional investors often trade them in similar market conditions.\n",
        "\n",
        "From analyzing the scatter plot above, it appears that there isn't a strongly visible pattern between Volume_GOOG and Close_MSFT. The points are somewhat dispersed, and while there may be a weak negative correlation (as indicated by the coefficient of -0.2781), it is not immediately clear from the visualization.\n",
        "\n",
        "Additionally, Close_AAPL (represented by color) Close_AAPL values (red shades) are more concentrated in the upper part of the plot where Close_MSFT is higher, and lower Close_AAPL values (blue shades) are more concentrated at the bottom where Close_MSFT is lower.\n",
        "\n",
        "To better understand the relationship, adding a trend line in the next graph would help visualize the general direction and strength of the correlation. This will make it easier to determine whether Higher Volume_GOOG is actually associated with lower Close_MSFT and whether the relationship is significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "PC_DDbJgDdxI",
        "outputId": "884935b3-1b4c-45b7-d6ce-11e11bb6df7d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "# Ensure df_jg is already created using your previous code\n",
        "\n",
        "# Define relevant columns\n",
        "volume_goog = \"Volume_GOOG\"\n",
        "close_msft = \"Close_MSFT\"\n",
        "close_appl = \"Close_AAPL\"\n",
        "\n",
        "# Drop missing values\n",
        "df_clean = df_jg[[volume_goog, close_msft, close_appl]].dropna()\n",
        "\n",
        "# Compute correlation coefficients\n",
        "corr_volume_close_msft, _ = spearmanr(df_clean[volume_goog], df_clean[close_msft])\n",
        "corr_volume_close_appl, _ = spearmanr(df_clean[volume_goog], df_clean[close_appl])\n",
        "corr_msft_appl, _ = spearmanr(df_clean[close_msft], df_clean[close_appl])\n",
        "\n",
        "print(f\"Spearman correlation between {volume_goog} and {close_msft}: {corr_volume_close_msft:.4f}\")\n",
        "print(f\"Spearman correlation between {volume_goog} and {close_appl}: {corr_volume_close_appl:.4f}\")\n",
        "print(f\"Spearman correlation between {close_msft} and {close_appl}: {corr_msft_appl:.4f}\")\n",
        "\n",
        "# Create a scatterplot with regression trend line\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(\n",
        "    df_clean[volume_goog],\n",
        "    df_clean[close_msft],\n",
        "    c=df_clean[close_appl],\n",
        "    cmap='coolwarm',\n",
        "    edgecolor='k',\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.colorbar(label='Close_AAPL')\n",
        "\n",
        "# Add trend line using seaborn's regression plot\n",
        "sns.regplot(\n",
        "    x=df_clean[volume_goog],\n",
        "    y=df_clean[close_msft],\n",
        "    scatter=False,  # Hide seaborn scatter plot\n",
        "    line_kws={\"color\": \"black\", \"linewidth\": 2, \"linestyle\": \"--\"}  # Trend line style\n",
        ")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Volume_GOOG\")\n",
        "plt.ylabel(\"Close_MSFT\")\n",
        "plt.title(\"Multivariate Scatter Plot with Trend Line: Volume_GOOG vs Close_MSFT (colored by Close_AAPL)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukEshORWDvBN"
      },
      "source": [
        "The trend line confirms a weak negative correlation between Volume_GOOG and Close_MSFT, but the relationship is not strong or significant.\n",
        "Close_AAPL still shows a clear positive correlation with Close_MSFT.\n",
        "Given the spread of data and confidence interval, we fail to reject the hypothesis that higher Volume_GOOG correlates with lower Close_MSFT—the evidence is not strong enough to support it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_W5TV7YfyWY"
      },
      "source": [
        "## Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LpAqQY_Hf5_p",
        "outputId": "6763fd93-d79c-4447-b2f9-2c26313da35c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Identify columns related to closing prices\n",
        "close_columns = [col for col in df_jg.columns if \"Close\" in col]\n",
        "\n",
        "# Check if there are any closing price columns\n",
        "if not close_columns:\n",
        "    print(\"No closing price data found in df_jg.\")\n",
        "else:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot closing prices for all available stocks\n",
        "    for col in close_columns:\n",
        "        plt.plot(df_jg.index, df_jg[col], label=col.replace(\"Close_\", \"\"))  # Remove 'Close_' for cleaner labels\n",
        "\n",
        "    # Graph customization\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Closing Price\")\n",
        "    plt.title(\"Time Series of Stock Closing Prices in df_jg\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZ7DRTbssuR"
      },
      "source": [
        "1. Upward trends suggest stocks are performing well.\n",
        "Downward trends indicate declining stock performance.\n",
        "Sideways trends (flat) suggest a consolidation phase.\n",
        "2. Highly volatile stocks (rapid ups and downs) may be risky.\n",
        "Smooth trends indicate stable price movement.\n",
        "3. If one stock outperforms others consistently, it might be a stronger investment.\n",
        "Divergences (one stock rising while others fall) might indicate sector rotation or company-specific events.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "437XmgwEf7jp",
        "outputId": "ff16e4f4-bfe7-4451-c264-e875d8ea78ce"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Identify unique tickers based on column names\n",
        "tickers = {col.split('_')[-1] for col in df_jg.columns if \"Close_\" in col}\n",
        "\n",
        "# Loop through each ticker and plot its closing price separately\n",
        "for ticker in sorted(tickers):  # Sorting for consistency\n",
        "    close_col = f\"Close_{ticker}\"\n",
        "\n",
        "    # Check if the column exists\n",
        "    if close_col in df_jg.columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(df_jg.index, df_jg[close_col], label=ticker, color='b')\n",
        "\n",
        "        # Graph customization\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Closing Price\")\n",
        "        plt.title(f\"Time Series of {ticker} Closing Prices\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Show the plot for each stock\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Skipping {ticker}, as closing price data is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBkUtK0qs3vi"
      },
      "source": [
        "1.  MSFT (Microsoft Corp.)\n",
        "\n",
        "  - Trend: Sideways with a mild uptrend\n",
        "\n",
        "  - Volatility: Low\n",
        "\n",
        "  - Insights: Microsoft remains strong due to AI and cloud expansion However, the stock is consolidating and needs a breakout catalyst\n",
        "\n",
        "2. AAPL (Apple Inc.)\n",
        "\n",
        "  - Trend: Strong uptrend\n",
        "\n",
        "  - Volatility: Low to moderate\n",
        "\n",
        "  - Insights: Apple’s growth is driven by iPhone sales and high-margin services. The stock remains a stable long-term investment\n",
        "\n",
        "3. META (Meta Platforms Inc.)\n",
        "\n",
        "  - Trend: Uptrend with a recent pullback\n",
        "\n",
        "  - Volatility: High\n",
        "\n",
        "  - Insights: Meta is benefiting from AI and digital advertising, but heavy spending on the Metaverse creates uncertainty\n",
        "\n",
        "4. AMZN (Amazon Inc.)\n",
        "\n",
        "  - Trend: Gradual uptrend\n",
        "\n",
        "  - Volatility: Moderate\n",
        "\n",
        "5. JNJ (Johnson & Johnson)\n",
        "\n",
        "  - Trend: Slight downtrend or sideways movement\n",
        "\n",
        "  - Volatility: Low\n",
        "\n",
        "6. GS (Goldman Sachs Group Inc.)\n",
        "\n",
        "  - Trend: Uptrend with fluctuations\n",
        "\n",
        "  - Volatility: Moderate\n",
        "\n",
        "7. UAL (United Airlines Holdings Inc.)\n",
        "\n",
        "  - Trend: Choppy with periodic uptrends\n",
        "\n",
        "  - Volatility: High\n",
        "\n",
        "8. TSLA (Tesla Inc.)\n",
        "\n",
        "  - Trend: Downtrend with volatility\n",
        "\n",
        "  - Volatility: Very high\n",
        "\n",
        "9. MA (Mastercard Inc.)\n",
        "\n",
        "  - Trend: Strong uptrend\n",
        "\n",
        "  - Volatility: Low\n",
        "\n",
        "10. GOOG (Alphabet Inc.)\n",
        "\n",
        "  - Trend: Uptrend with stability\n",
        "  \n",
        "  - Volatility: Low"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDEN6oH8tJ_l"
      },
      "source": [
        "- Tech Stocks (MSFT, AAPL, META, AMZN, GOOG): Strong AI and cloud growth, with META being the most volatile\n",
        "- Finance & Payments (GS, MA): MA benefits from digital payments, while GS gains from interest rates but faces economic risks\n",
        "- Cyclical Stocks (UAL, TSLA): UAL is dependent on travel demand, and TSLA is volatile due to market competition\n",
        "- (JNJ): Stable but faces legal risks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8jeBkds6qROV",
        "outputId": "141cea4a-3a69-4dc5-d622-89aaa14379d7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract tickers\n",
        "tickers = {col.split('_')[-1] for col in df_jg.columns if 'Close_' in col}\n",
        "\n",
        "for ticker in tickers:\n",
        "    buy_col = f\"Buy_{ticker}\"\n",
        "    sell_col = f\"Sell_{ticker}\"\n",
        "\n",
        "    if buy_col in df_jg.columns and sell_col in df_jg.columns:\n",
        "        plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "\n",
        "        plt.plot(df_jg.index, df_jg[buy_col], label='Buy', color='green')\n",
        "        plt.plot(df_jg.index, df_jg[sell_col], label='Sell', color='red')\n",
        "\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.title(f\"Buy/Sell Trends for {ticker}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Buy or Sell column not found for {ticker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOKnAAVUuRX7"
      },
      "source": [
        "- Stable stocks (JNJ, MA, GOOG) have low volatility in buy/sell activity, indicating long-term holding strategies.\n",
        "- Tech stocks (AAPL, MSFT, META) show predictable buy patterns before events and sell-offs after price surges.\n",
        "- Speculative stocks (TSLA, UAL, GS) have erratic buy/sell signals, reflecting high volatility and trading speculation.\n",
        "- Financial stocks (GS, MA) show different behaviors: GS is fast-trading, while MA is stable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KARhiUzegBue",
        "outputId": "da4347da-497c-41c1-a0d9-4bec49e6487d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Extract tickers from column names\n",
        "tickers = {col.split('_')[-1] for col in df_jg.columns if \"Close_\" in col}\n",
        "\n",
        "# Loop through each stock to perform seasonal decomposition\n",
        "for ticker in sorted(tickers):\n",
        "    close_col = f\"Close_{ticker}\"\n",
        "\n",
        "    # Check if the column exists\n",
        "    if close_col in df_jg.columns:\n",
        "        df_stock = df_jg[[close_col]].dropna()  # Select only the relevant column and drop NaNs\n",
        "        df_stock = df_stock.rename(columns={close_col: \"Close\"})  # Rename for consistency\n",
        "\n",
        "        # Perform seasonal decomposition\n",
        "        try:\n",
        "            decomposition = seasonal_decompose(df_stock[\"Close\"], model=\"additive\", period=30)  # Adjust period as needed\n",
        "\n",
        "            # Plot decomposition results\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            plt.subplot(411)\n",
        "            plt.plot(df_stock[\"Close\"], label=\"Original Data\", color=\"blue\")\n",
        "            plt.legend(loc=\"upper left\")\n",
        "\n",
        "            plt.subplot(412)\n",
        "            plt.plot(decomposition.trend, label=\"Trend\", color=\"green\")\n",
        "            plt.legend(loc=\"upper left\")\n",
        "\n",
        "            plt.subplot(413)\n",
        "            plt.plot(decomposition.seasonal, label=\"Seasonality\", color=\"orange\")\n",
        "            plt.legend(loc=\"upper left\")\n",
        "\n",
        "            plt.subplot(414)\n",
        "            plt.plot(decomposition.resid, label=\"Residuals (Irregular Component)\", color=\"red\")\n",
        "            plt.legend(loc=\"upper left\")\n",
        "\n",
        "            plt.suptitle(f\"Seasonal Decomposition for {ticker}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {ticker} due to decomposition error: {e}\")\n",
        "    else:\n",
        "        print(f\"Skipping {ticker}, as closing price data is missing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioF7-qAjto8i"
      },
      "source": [
        "1. MSFT (Microsoft Corp.)\n",
        "\n",
        "  - Trend: Upward, with periods of consolidation.\n",
        "\n",
        "  - Seasonality: Slight cyclic movements every 30 days.\n",
        "\n",
        "  - Residuals: Some unexpected fluctuations but mostly stable.\n",
        "\n",
        "  - Insights: MSFT benefits from cloud and AI trends, keeping its stock in an uptrend with minor seasonal effects.\n",
        "\n",
        "2. AAPL (Apple Inc.)\n",
        "\n",
        "  - Trend: Strong uptrend.\n",
        "\n",
        "  - Seasonality: Noticeable peaks around earnings announcements.\n",
        "\n",
        "  - Residuals: Occasional sharp drops, likely linked to product launches and market reactions.\n",
        "\n",
        "  - Insights: Apple’s stock remains steady, with seasonality influenced by new product releases and earnings cycles.\n",
        "\n",
        "3. META (Meta Platforms Inc.)\n",
        "\n",
        "  - Trend: Increasing, but with high volatility.\n",
        "\n",
        "  - Seasonality: A repeating pattern, potentially tied to advertising revenue fluctuations.\n",
        "\n",
        "  - Residuals: Larger than other tech stocks, suggesting uncertainty.\n",
        "\n",
        "  - Insights: META’s heavy investments in AI and Metaverse cause fluctuations, but ad revenue growth supports long-term stability.\n",
        "\n",
        "4. AMZN (Amazon Inc.)\n",
        "\n",
        "  - Trend: Gradual uptrend.\n",
        "\n",
        "  - Seasonality: Strong quarterly cycles, reflecting retail demand (holiday sales).\n",
        "\n",
        "  - Residuals: Spikes around Q4, confirming the impact of holiday shopping.\n",
        "\n",
        "  - Insights: AMZN’s stock follows retail seasonality, with clear uptrends in Q4 due to Black Friday and holiday sales.\n",
        "\n",
        "5. JNJ (Johnson & Johnson)\n",
        "\n",
        "  - Trend: Slight downward movement.\n",
        "\n",
        "  - Seasonality: Minimal but present, likely tied to pharmaceutical cycles.\n",
        "\n",
        "  - Residuals: Low volatility, making it relatively stable.\n",
        "\n",
        "  - Insights: JNJ is a defensive stock, with little seasonal impact but a recent downward trend due to legal challenges.\n",
        "\n",
        "6. GS (Goldman Sachs Group Inc.)\n",
        "\n",
        "  - Trend: Upward but inconsistent.\n",
        "\n",
        "  - Seasonality: Noticeable cycles, possibly linked to earnings and interest rate changes.\n",
        "\n",
        "  - Residuals: High fluctuations during financial crises.\n",
        "\n",
        "  - Insights: GS is influenced by market cycles, interest rates, and economic conditions, making it moderately seasonal.\n",
        "\n",
        "7. UAL (United Airlines Holdings Inc.)\n",
        "\n",
        "  - Trend: Choppy, with a long-term uptrend.\n",
        "\n",
        "  - Seasonality: Strong, reflecting travel demand (peaks in summer and holidays).\n",
        "\n",
        "  - Residuals: Large unexpected fluctuations, likely due to fuel costs and travel restrictions.\n",
        "\n",
        "  - Insights: UAL’s stock is highly seasonal, peaking during travel seasons and facing risks from oil price volatility.\n",
        "\n",
        "8. TSLA (Tesla Inc.)\n",
        "\n",
        "  - Trend: Volatile, recently declining.\n",
        "\n",
        "  - Seasonality: Some patterns, potentially influenced by delivery reports.\n",
        "\n",
        "  - Residuals: High unpredictability due to Elon Musk’s actions and market sentiment.\n",
        "\n",
        "  - Insights: TSLA faces high speculation, making seasonal patterns less reliable. Delivery announcements and earnings cause major swings.\n",
        "\n",
        "9. MA (Mastercard Inc.)\n",
        "\n",
        "  - Trend: Strong upward trend.\n",
        "\n",
        "  - Seasonality: Noticeable, with spending cycles (e.g., holiday shopping boosts Q4).\n",
        "\n",
        "  - Residuals: Small, indicating a stable company.\n",
        "\n",
        "  - Insights: MA benefits from global payment trends, with slight seasonal effects due to consumer spending habits.\n",
        "\n",
        "10. GOOG (Alphabet Inc.)\n",
        "\n",
        "  - Trend: Upward and stable.\n",
        "\n",
        "  - Seasonality: Moderate, linked to ad revenue and tech trends.\n",
        "\n",
        "  - Residuals: Some volatility, often tied to regulatory news.\n",
        "\n",
        "  - Insights: GOOG remains stable with minor seasonal dips during weaker ad revenue periods but overall strong growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmvPIMu4dcRH"
      },
      "source": [
        "## Modeling -- BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcxIFpZmPvRG"
      },
      "source": [
        "Calculating the Adj Close Average for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXBURaffdjD2",
        "outputId": "635855ca-63cb-4a2a-90a6-d0bcd297aa18"
      },
      "outputs": [],
      "source": [
        "from pyomo.environ import *\n",
        "import pandas as pd\n",
        "\n",
        "# Identify all columns that contain \"Adj Close_\"\n",
        "adj_cols = [col for col in df_jg.columns if col.startswith(\"Adj Close\")]\n",
        "\n",
        "# Extract tickers (assuming format \"Adj Close_{ticker}\")\n",
        "tickers = [col.split(\"_\")[1] for col in adj_cols]\n",
        "\n",
        "# Loop through each ticker and calculate the average adjusted closing price\n",
        "avg_adj_close = {}  # Use a dictionary to store the results\n",
        "for ticker in tickers:\n",
        "    avg_adj_close[ticker] = df_jg[f\"Adj Close_{ticker}\"].mean()\n",
        "\n",
        "# Print the result\n",
        "print(\"Average Adj Closing Price per stock:\")\n",
        "for ticker, avg_price in avg_adj_close.items():\n",
        "    print(f\"{ticker}: {avg_price}\")\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "df_jg_returns = pd.DataFrame(list(avg_adj_close.items()), columns=[\"Ticker\", \"Average_Adj_Close\"])\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(\"\\ndf_jg_returns:\")\n",
        "print(df_jg_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCJHCYtoPzGh"
      },
      "source": [
        "Creating the covaraince matrix from the Average of Adj Close for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiW3Cu7sdsx2"
      },
      "outputs": [],
      "source": [
        "# Select closing prices for all relevant tickers:\n",
        "close_prices_all = df_jg[[f\"Adj Close_{ticker}\" for ticker in tickers]]\n",
        "\n",
        "# Calculate the covariance matrix:\n",
        "df_jg_cov = close_prices_all.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSZ-aeQsduzG"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGAc6E0idwdr"
      },
      "outputs": [],
      "source": [
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5-AJged3Zw"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH75JPvWd0Qh"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o1AHKHIeJil"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yip0Vp73eLUO",
        "outputId": "309a0639-0425-4006-a1a6-3cfdef4601c8"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFKFeP3qeONj",
        "outputId": "87b4f28b-b10b-4717-e465-ad1fe3fa6f7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from pylab import *\n",
        "\n",
        "import shutil\n",
        "import sys\n",
        "import os.path\n",
        "\n",
        "if not shutil.which(\"pyomo\"):\n",
        "    !pip install -q pyomo\n",
        "    assert(shutil.which(\"pyomo\"))\n",
        "\n",
        "if not shutil.which(\"ipopt\"):\n",
        "    # here is the IPOPT zip file\n",
        "    !gdown 10XRvLZqrpSNiXVAN-pipU52BVRwoGcNQ\n",
        "    !unzip -o -q ipopt-linux64_dw\n",
        "    assert(shutil.which(\"ipopt\") or os.path.isfile(\"ipopt\"))\n",
        "\n",
        "from pyomo.environ import *\n",
        "\n",
        "SOLVER = 'ipopt'\n",
        "EXECUTABLE = '/content/ipopt'\n",
        "ipopt_executable = '/content/ipopt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdtKLc1oeRFK",
        "outputId": "ce44560f-f306-4c53-a713-0a74f824ee5e"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bnC8EF0ceWgk",
        "outputId": "3bd09c25-2980-40c3-ba56-c0c067e6a027"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling BASELINE: Optimal Stock Allocation for Different Risk Levels') # corrected typo\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling BASELINE: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling BASELINE: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "P4IzqyiYxrJ9",
        "outputId": "db1e3527-8d42-45e3-c698-0ab177c92d64"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter data for the specified risk range\n",
        "filtered_risk = [r for r in risk if 0.042 <= r <= 0.05]\n",
        "filtered_reward = [reward[risk.index(r)] for r in filtered_risk]\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(filtered_risk, filtered_reward, '-.')\n",
        "plt.title('Efficient Frontier (Risk 0.042 - 0.05)')\n",
        "plt.xlabel('Risk')\n",
        "plt.ylabel('Reward (Return)')\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuUrfu7dxv6K"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (seperate optimal stock allocation graphs and a stacked bar graph for each stock for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.05).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- MA, AMZN, AAPL, UAL, GS, TSLA, and META are included in the portfolio typically at around risk level 0.0388 to 0.0443 (higher end of the range) but at a miniscule proportion.\n",
        "\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at a risk level of 0.0428 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNl13nMGtMgd"
      },
      "source": [
        "### Ranking of Optimal Stock Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayi99XAcxtMg",
        "outputId": "80a62212-5d9f-414c-a0ed-ef802dcf47fd"
      },
      "outputs": [],
      "source": [
        "# Choose a specific risk limit (for example, the maximum risk limit in your analysis)\n",
        "selected_risk = param_analysis.index.max()\n",
        "optimal_allocations = param_analysis.loc[selected_risk]\n",
        "\n",
        "# Sort the allocations in ascending order\n",
        "sorted_allocations = optimal_allocations.sort_values()\n",
        "\n",
        "# Print each stock with its allocation percentage\n",
        "print(f\"Optimal Allocation (sorted ascending) for risk limit {selected_risk}:\")\n",
        "for stock, allocation in sorted_allocations.items():\n",
        "    print(f\"{stock}: {allocation*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZYoOwbDN68d"
      },
      "source": [
        "This confirms that JNJ dominates the portfolio more than any other stock by x3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgdd4KXLyf2i"
      },
      "source": [
        "## Modeling w/o JNJ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOd8OB-52wgw"
      },
      "source": [
        "Our baseline model indicated that JNJ was dominating the portfolio. To explore alternatives, we’re removing JNJ from the decision variables while keeping all other conditions the same, allowing us to see which stock becomes the next most dominant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4aw7XBpyh2s"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufanNWg1IMD"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkTnveLYykpG"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe9Qv09Ry2Hi"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge6zxTvKy4Vd",
        "outputId": "8ec48d4b-214d-4286-cb4d-4b4ae043d4e6"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Xpu6Koy6Fs",
        "outputId": "813bb2ef-5f21-4f15-fa20-9ecd9c351bfb"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kpMubua_y_Wp",
        "outputId": "80e48365-0d91-4df8-8226-63fd280fdee4"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o JNJ: Optimal Stock Allocation for Different Risk Levels') # corrected typo\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(3, 3),       # 3 rows, 3 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o JNJ: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o JNJ: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyTWK44X2z4n"
      },
      "source": [
        "After taking out JNJ, the next most dominate stock is MSFT which is expected when you look at the BASELINE model. It was either going to be GOOD or MSFT.\n",
        "  - We are also starting to see other stocks becoming more dominate in the Optimal Stock Allocation like UAL and GOOG.\n",
        "  - Without JNJ, the maximum return has dropped down to 250."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nR4Li7TzCfY"
      },
      "source": [
        "## Modeling w/o MSFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzCxLymv3oZL"
      },
      "source": [
        "After removing JNJ, MSFT emerged as the most dominant stock. To identify the next most influential asset, we are now excluding both MSFT and JNJ from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEgOfyKOzD-g"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpA9bmW1gzX"
      },
      "source": [
        "MAX(Return = 210*AAPL + 527*META + 190*AMZN + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le7cYiEezGdd"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89zry29ezIbY"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR3XRcT2zKVM",
        "outputId": "1610392d-125a-42a1-89eb-6c6faea966f0"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.AAPL, m.META, m.AMZN, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = [ 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rQm-O5mzMFK",
        "outputId": "b25cbedb-9d29-4398-bffe-55bcb4b7b3e6"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.AAPL(), m.META(), m.AMZN(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] = m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R-NGddgEzO6J",
        "outputId": "603fd0de-da19-444e-b573-03e8387ab1e3"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['AAPL', 'META', 'AMZN', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o MSFT: Optimal Stock Allocation for Different Risk Levels') # corrected typo\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(4, 2),       # 4 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o MSFT: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o MSFT: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7W818G5Rvk"
      },
      "source": [
        "After taking out MSFT, the next most dominate stock is GOOG which is expected when you look at the BASELINE model. GOOG, MSFT, and JNJ looked like they were the top 3 most dominate stocks at the maximum risk level of 5%.\n",
        "  - Without JNJ, the maximum return has exponentially dropped to 2% which indicates that JNJ and MSFT have a significant impact in the portfolios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgOacflyzRfX"
      },
      "source": [
        "## Modeling w/o GOOG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFfhO_pg4GkT"
      },
      "source": [
        "After removing JNJ and MSFT, GOOG emerged as the most dominant stock. To identify the next most influential asset, we are now excluding MSFT, JNJ, and GOOG from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8pKRcDGzTrc"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVsP3pfd1ju3"
      },
      "source": [
        "MAX(Return = 210*AAPL + 527*META + 190*AMZN + 482*GS + 62*UAL + 249*TSLA + 481*MA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvg7xjh3zWE3"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA, sense = maximize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKF1mFzszXyv"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.AAPL + m.META + m.AMZN + m.GS + m.UAL + m.TSLA + m.MA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIgn_TztzZhp",
        "outputId": "c122f373-ed5f-4ed1-a104-af8a3ee5b5f4"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.AAPL, m.META, m.AMZN, m.GS, m.UAL, m.TSLA, m.MA]\n",
        "  tickers = [ 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBp8Xtykzbcu",
        "outputId": "51abf782-a96a-4210-da05-3aa7355f57b0"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.AAPL(), m.META(), m.AMZN(), m.GS(), m.UAL(), m.TSLA(), m.MA()]\n",
        "  returns[r] = m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T7KyrtPZzd6g",
        "outputId": "30115c88-1f47-4ec2-9c32-2ab6ccb992d5"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['AAPL', 'META', 'AMZN', 'GS', 'UAL', 'TSLA', 'MA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o GOOG: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o GOOG: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o GOOG: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehclfs8H5ywJ"
      },
      "source": [
        "After taking out GOOG, the next most dominant stock is AMZN.\n",
        "  - We are also starting to see other stocks, such as AAPL, become more prominent in the Optimal Stock Allocation.\n",
        "  - Without GOOG, the maximum return has stayed at 2%, which means that the portfolio's return potential remains the same even after removing one of its previously dominant stock.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHKxZHHzgj6"
      },
      "source": [
        "## Modeling w/o AMZN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4OGUf34PqE"
      },
      "source": [
        "After removing JNJ, GOOG, and MSFT, AMZN emerged as the most dominant stock. To identify the next most influential asset, we are now also going to exclude AMZN from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JczMttEQzjK5"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRb_Nfg91n6T"
      },
      "source": [
        "MAX(Return = 210*AAPL + 527*META + 482*GS + 62*UAL + 249*TSLA + 481*MA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKopX51PzmDa"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA, sense = maximize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oltn2qjzpAG"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.AAPL + m.META + m.GS + m.UAL + m.TSLA + m.MA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.AAPL + m.META + m.GS + m.UAL + m.TSLA + m.MA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdvoq-uDzqk8",
        "outputId": "cf0c5b1d-a0a8-4372-d91c-940130594bef"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.AAPL, m.META, m.GS, m.UAL, m.TSLA, m.MA]\n",
        "  tickers = [ 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy-sfB0kztGa",
        "outputId": "881de12e-e146-46ff-c72a-6e6798c144dc"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.AAPL(), m.META(), m.GS(), m.UAL(), m.TSLA(), m.MA()]\n",
        "  returns[r] = m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rgt-JtLrzw5_",
        "outputId": "300a2ac0-043d-435d-9a8c-7f8c41190cc7"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['AAPL', 'META', 'GS', 'UAL', 'TSLA', 'MA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o AMZN: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o AMZN: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o AMZN: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JWxKxbC6yWd"
      },
      "source": [
        "After taking out AMZN, the next most dominant stock is UAL.\n",
        "  - We are also starting to see other stocks, such as UAL, become more prominent in the Optimal Stock Allocation.\n",
        "  - Without AMZN, the maximum return has dropped to 1% indicating that without the previous dominating stocks the return could keep getting lower.\n",
        "  - We are starting to see which stocks are not being incorporated in the optimized stock allocation at all like META, GS, TSLA, and MA which could be indicating that under the current model parameters and constraints, these stocks do not contribute favorably to the portfolio's overall risk-return balance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FQ5U2PHzy-0"
      },
      "source": [
        "## Modeling w/o UAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhrC96xP4aep"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, and AMZN from the decision variables, UAL emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxnf1cRNzxii"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfAGiYcw1rtS"
      },
      "source": [
        "MAX(Return = 210*AAPL + 527*META + 482*GS + 249*TSLA + 481*MA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDAd1uUb1y2m"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.AAPL + m.META + m.GS + m.TSLA + m.MA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.AAPL + m.META + m.GS + m.TSLA + m.MA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV7GB0qdz5tW",
        "outputId": "ebc54e5a-86de-4e6a-c9f3-8faf7e2cd847"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.AAPL, m.META, m.GS, m.TSLA, m.MA]\n",
        "  tickers = [ 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_GS', 'Adj Close_TSLA', 'Adj Close_MA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsesNLc8z9P7",
        "outputId": "86683706-75c6-4dd3-9d1d-356f56923414"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.AAPL(), m.META(), m.GS(), m.TSLA(), m.MA()]\n",
        "  returns[r] = m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LFV-yKbCz_2m",
        "outputId": "d4bf68f7-7f96-4424-f173-b9c2d83977e1"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['AAPL', 'META', 'GS', 'TSLA', 'MA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o UAL: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o UAL: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o UAL: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76D54cFX8Czw"
      },
      "source": [
        "After taking out UAL, the next most dominant stock is APPL.\n",
        "  - Without UAL, the maximum return has increased back to 2% indicating that UAL was constraining the portfolio's return potential and that its removal allows for a little more efficient allocation of assets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIKNQRh40B3S"
      },
      "source": [
        "## Modeling w/o AAPL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_WWQfRK400Q"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, AMZN, and UAL from the decision variables, AAPL emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6aNt5Vo0DQf"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMIsQyNO12Ea"
      },
      "source": [
        "MAX(Return = 527*META + 482*GS + 249*TSLA + 481*MA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTqZQrz715SS"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.META + m.GS + m.TSLA + m.MA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.META + m.GS + m.TSLA + m.MA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca_QTtdx0E-e",
        "outputId": "479b85d1-8355-4afb-a8bf-5803106dbfb3"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.META, m.GS, m.TSLA, m.MA]\n",
        "  tickers = [ 'Adj Close_META', 'Adj Close_GS', 'Adj Close_TSLA', 'Adj Close_MA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgbz39k20K5p",
        "outputId": "1169f137-9149-4e1f-dfef-2229feb05efc"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.META(), m.GS(), m.TSLA(), m.MA()]\n",
        "  returns[r] = m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Up8sUrop0Nn1",
        "outputId": "2c12a767-b40f-4e4e-e661-3824afe28e5d"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['META', 'GS', 'TSLA', 'MA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o AAPL: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o AAPL: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o AAPL: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW84o0sD8dRE"
      },
      "source": [
        "After taking out AAPL, the next most dominant stock is MA.\n",
        "  - Without AAPL, the maximum return has increased to 2.5% indicating that AAPL was constraining the portfolio's return potential and that its removal allows for a little more efficient allocation of assets.  \n",
        "  - We are also seeing that other than MA, the other stocks are not being incorporated which could be indicating that under the current model parameters, only a very limited set of stocks contributes meaningfully to optimizing the risk-return balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX4znnYA0R37"
      },
      "source": [
        "## Modeling w/o MA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl0x1Kq45ED"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, AMZN, UAL, and AAPL from the decision variables, MA emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq4_VKVJ0UfC"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoGbYTF2ArQ"
      },
      "source": [
        "MAX(Return = 527*META + 482*GS + 249*TSLA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VRxtwY31_bB"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.META + m.GS + m.TSLA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.META + m.GS + m.TSLA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0nQLdy60aw_",
        "outputId": "304f0198-ee2a-4f73-b721-53b962e5469d"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.META, m.GS, m.TSLA]\n",
        "  tickers = [ 'Adj Close_META', 'Adj Close_GS', 'Adj Close_TSLA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv0asBLD0cjn",
        "outputId": "abfd0c39-edf1-4fbb-8bc2-0c1be597f986"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.META(), m.GS(), m.TSLA()]\n",
        "  returns[r] = m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zXlMSQDt0gMa",
        "outputId": "32e055d9-3c8f-4a90-c108-3dfa514420f5"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['META', 'GS', 'TSLA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o MA: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o MA: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o MA: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NIXHEDS9FQj"
      },
      "source": [
        "After taking out MA, the next most dominant stock is META.\n",
        "  - Without MA, the maximum return has dropped significantly to 1.2% indicating that MA is a critical driver in the portfolio, essential for achieving higher return potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi_-dhJO0gw3"
      },
      "source": [
        "## Modeling w/o META"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btzgjTRO4-CC"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, AMZN, UAL, AAPL, and MA from the decision variables, META emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYMZvSdB0i-x"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9ZDHw8m2ILQ"
      },
      "source": [
        "MAX(Return = 482*GS + 249*TSLA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ydzwCR2H-R"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.GS + m.TSLA == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.GS + m.TSLA == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDlu4jxz0m2X",
        "outputId": "9ef208c8-66f2-4769-a6a4-a169b6394d84"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.GS, m.TSLA]\n",
        "  tickers = ['Adj Close_GS', 'Adj Close_TSLA']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VAR9u6q0ok3",
        "outputId": "bea1d536-2791-49e0-c8ba-18693f3c01a4"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.GS(), m.TSLA()]\n",
        "  returns[r] = m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lwVJ8Tb80qTl",
        "outputId": "85504726-2497-4245-9d10-7c2cb746c5ab"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['GS', 'TSLA']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o META: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o META: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o META: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBkQXKn9jQa"
      },
      "source": [
        "After taking out META, the next most dominant stock is TSLA.\n",
        "  - Without META, the maximum return has dropped by a little to 1% indicating that META was contributing meaningfully to the portfolio's return potential.\n",
        "  - Additionally, the optimized allocations for TSLA and GS are very close to each other, suggesting that both stocks are nearly equally effective in driving the portfolio's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFZDn4Nc0tDo"
      },
      "source": [
        "## Modeling w/o TSLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdbfP3jW5D9v"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, AMZN, UAL, AAPL, MA, and META from the decision variables, TSLA emerged as the most dominant stock. We are going to analyze how GS does by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho2qLyFV0vac"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PokQjTmU2fsO"
      },
      "source": [
        "MAX(Return = 482*GS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ2hqARJ0xMN"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS, sense = maximize)\n",
        "\n",
        "m.total_risk = Constraint(expr = m.GS == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.GS == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br4xLpFk0y_V",
        "outputId": "1889c3f9-7ccb-4a5f-b5a7-86c92daddddf"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.GS]\n",
        "  tickers = ['Adj Close_GS']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx2UOmTF00yL",
        "outputId": "905926e6-d5eb-4019-a317-db39f746cd74"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.GS()]\n",
        "  returns[r] = m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CHLSHerm02mo",
        "outputId": "64d63e9b-fabf-43fa-acdb-40bf8e92dec3"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['GS']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o TSLA: Optimal Stock Allocation for Different Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Remove or comment out the for loop and directly plot on 'axes'\n",
        "param_analysis.iloc[:, 0].plot(ax=axes, legend=True)  # Plot the first (and only) column on the axes\n",
        "axes.set_xlabel(\"Risk Level\")  # Set x-axis label for the axes\n",
        "axes.set_ylabel(\"Optimal Allocation\")  # Set y-axis label for the axes\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o TSLA: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o TSLA: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMqJFOs-Qk2"
      },
      "source": [
        "After removing all decision variables except for GS, the maximum return drops to 1.2%, highlighting that without the dominant stocks, the portfolio’s return potential collapses dramatically—from 300% to just 1.2%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140LeLjrl1tX"
      },
      "source": [
        "## Modeling 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRNp3sXml3LA"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ePr13Vl6Eh"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns2RlbVF2k7L"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnT5HJLil8eu"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.10)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJDoBbu1l-O2",
        "outputId": "6e15d44d-a057-4ba2-ce37-fdc8f9196338"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.10\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.001) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzeYcIk_mA2d",
        "outputId": "1d64c13f-3fd5-46eb-b4ba-6c2581f9e110"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LNwJW8hNmF7C",
        "outputId": "1f6128c6-6412-48ea-8748-328c9e75fc4e"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 10%: Optimal Stock Allocation for Different Risk Levels') # corrected typo\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 10%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk: {risk}\") # added colon\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward: {reward}\") # added colon\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 10%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Tm1e6hx3WE"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.1).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- META is included in the portfolio only at the risk level of 0.0813 but at a miniscule proportion compared to JNJ.\n",
        "- AMZN, AAPL, UAL, GS, MA, and TSLA are included twice in the portfolio, once at the risk levels of 0.0583 and 0.0813, also at a miniscule proportion compared to JNJ.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at a risk level of 0.0813 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHx4omEOeu4D"
      },
      "source": [
        "## Modeling 25%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7HAzkkee3cv"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "870dwNJ1e63X"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jkQu-bF2m04"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "640aCF4RfZE6"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.25)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP_rXz6QfbeD",
        "outputId": "f1a755e6-ce4f-4528-d88f-3fea49d493b7"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.25\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0025) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "docq-Dw7feMH",
        "outputId": "bd403f62-2ba8-48fd-d85b-ac6599a578fa"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-_4nAilsgneT",
        "outputId": "734160c0-1eaa-4d4e-9eff-37ddf2ff3dd1"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Model 25%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 10))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 25%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Model 25%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PaE4J-yx_5W"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.25).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- MA, META, GS, TSLA, UAL, AMZN, and AAPL are included in the portfolio typically at around risk level 0.1353 to 0.2053 (higher end of the range) but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.1978 and 0.2053 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yQHqnSFgpYc"
      },
      "source": [
        "## Modeling 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLFDpaCvgrR1"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_S6tj2AgtpL"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Jnd6zt2n3n"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcKCaT5ngy9v"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.50)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93jhWwSog_Q8",
        "outputId": "fae642ea-e421-423b-f7c3-68b5bc2f7b75"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.50\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.005) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPwhp5ckhFAR",
        "outputId": "ad865919-2d85-42d9-8f40-86beee49acb9"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCdr-Yr0hIBM",
        "outputId": "e8d4a731-c89a-4c14-d7ac-317edcc72ffd"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Model 50%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 50%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Model 50%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7MiS189yKIb"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.5).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- MA, META, GS, TSLA, UAL, AMZN, and AAPL are included in the portfolio at risk levels 0.1403 and 0.3403 but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.3403 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEkqiForhPpC"
      },
      "source": [
        "## Modeling 75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNaI-ClWhRJO"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv_BrKVWhTat"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0BUdjsM2o7O"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2aTWa4phXoZ"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.75)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_EOayQDhcve",
        "outputId": "a7bd308b-7e03-4e71-8f0c-13f418d16c13"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.75\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0075) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lquz932whef5",
        "outputId": "a08b86c6-86f3-4158-ed2c-2610f189d305"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MlgWcGC6hobB",
        "outputId": "0b8189d3-486d-446c-eab6-490c2327f8d8"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 75%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 75%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 75%The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TASnOVQjyRz0"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.75).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- MA, META, GS, TSLA, AMZN, UAL, and AAPL are included in the portfolio around risk levels 0.1203 to 0.3453 (lower end of the range) but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.3303 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Returns reached to 200 after increasing the maximum risk to 75%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28TW_QXMhpU1"
      },
      "source": [
        "## Modeling 95%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD0OOUazhr_7"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GS = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.TSLA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MA = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.UAL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AMZN = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.META = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjIV0yiihxPr"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_jg_returns[\"Average_Adj_Close\"].iloc[0] * m.MSFT +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[2] * m.META +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[3] * m.AMZN +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[4] * m.JNJ +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[5] * m.GS +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[6] * m.UAL +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[7] * m.TSLA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[8] * m.MA +\n",
        "                        df_jg_returns[\"Average_Adj_Close\"].iloc[9] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re7FUlOO2p-P"
      },
      "source": [
        "MAX(Return = 419*MSFT + 210*AAPL + 527*META + 190*AMZN + 152*JNJ + 482*GS + 62*UAL + 249*TSLA + 481*MA +168*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqTlgPSzhzgH"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 0.95)\n",
        "m.sum_proportion = Constraint(expr = m.MSFT + m.AAPL + m.META + m.AMZN + m.JNJ + m.GS + m.UAL + m.TSLA + m.MA + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YekCBYinh4t1",
        "outputId": "5b8a9f12-7c29-4efd-ce79-699690261ef0"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.MSFT, m.AAPL, m.META, m.AMZN, m.JNJ, m.GS, m.UAL, m.TSLA, m.MA, m.GOOG]\n",
        "  tickers = ['Adj Close_MSFT', 'Adj Close_AAPL', 'Adj Close_META', 'Adj Close_AMZN', 'Adj Close_JNJ', 'Adj Close_GS', 'Adj Close_UAL', 'Adj Close_TSLA', 'Adj Close_MA', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i]*df_jg_cov.at[tickers[i],tickers[j]]*variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk =  0.95\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0095) # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBOKaoYBh-fV",
        "outputId": "522b7558-7387-4473-c589-72b5c76980fd"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.MSFT(), m.AAPL(), m.META(), m.AMZN(), m.JNJ(), m.GS(), m.UAL(), m.TSLA(), m.MA(), m.GOOG()]\n",
        "  returns[r] =  m.MSFT()*df_jg_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_jg_returns[\"Average_Adj_Close\"].iloc[1] + m.META()*df_jg_returns[\"Average_Adj_Close\"].iloc[2] + m.AMZN()*df_jg_returns[\"Average_Adj_Close\"].iloc[3] + m.JNJ()*df_jg_returns[\"Average_Adj_Close\"].iloc[4] + m.GS()*df_jg_returns[\"Average_Adj_Close\"].iloc[5] + m.UAL()*df_jg_returns[\"Average_Adj_Close\"].iloc[6] + m.TSLA()*df_jg_returns[\"Average_Adj_Close\"].iloc[7] + m.MA()*df_jg_returns[\"Average_Adj_Close\"].iloc[8] + m.GOOG()*df_jg_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4OqqwMqNiCCk",
        "outputId": "fb7579e9-72d3-4a2b-ce4e-e6ee4f09da7e"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['MSFT', 'AAPL', 'META', 'AMZN', 'JNJ', 'GS', 'UAL', 'TSLA', 'MA', 'GOOG']]\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 95%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 95%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 95%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXdhiYtyaHd"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.95).\n",
        "\n",
        "- One stock (color purple) dominates most all of these bars which corresponds to the stock JNJ.\n",
        "  - This indicates that the model often allocates JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- MA, META, GS, TSLA, AMZN, UAL, and AAPL are included in the portfolio around risk levels 0.0573 to 0.8648 (more spread out compared to previoud models) but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.8648 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Returns reached to 300 after increasing the maximum risk to 95%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhUBiOHpPZY0"
      },
      "source": [
        "## Josh's Model Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T-oeF1AqhSV"
      },
      "source": [
        "\n",
        "\n",
        "- The series of Optimal Stock Allocation models demonstrates that the risk return relationship is highly non-linear (Non-Linear Optimization). Small changes in risk constraints can trigger dramatic shifts in asset allocation and, consequently, in expected returns.  \n",
        "- In our baseline models, a dominant stock (JNJ) consistently drives the portfolio's composition. Removing dominant stocks like JNJ, MSFT, and GOOG sequentially reveals how critical these assets are in achieving high returns.  \n",
        "- Lower risk models (the 5% model, baseline model) can theoretically achieve very high returns through narrow, specific allocations, but they tend to be highly sensitive and potentially volatile. In contrast, the 10% model (the optimal model) delivers up to 175% return with a more stable profile, making it a better option compared to the others with a overall lower risk and volatility.\n",
        "- Conversely, as the maximum allowed risk increases (25%, 50%, 75%, 95%), we observe that while the overall return potential may rise, the efficiency of the risk-return trade-off does not improve linearly.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-fMbcW7vU5"
      },
      "source": [
        "# Scott Franklin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk4zp_gol-Za"
      },
      "source": [
        "1. JPMorgan Chase (JPM)\n",
        "A leading global financial institution offering investment banking, asset management, and retail banking services.\n",
        "\n",
        "2. Apple (AAPL)\n",
        "Known for its consumer electronics (iPhone, iPad) and services (iCloud, Apple Music), a leader in tech innovation.\n",
        "\n",
        "3. Home Depot (HD)\n",
        "The largest home improvement retailer, benefiting from strong demand in DIY and home renovation products.\n",
        "\n",
        "4. Microsoft (MSFT)\n",
        "A tech giant in software (Windows, Office), cloud services (Azure), and gaming (Xbox), with steady growth.\n",
        "\n",
        "5. Visa (V)\n",
        "A global leader in digital payments, facilitating secure credit and debit transactions worldwide.\n",
        "\n",
        "6. Walmart (WMT)\n",
        "The world’s largest retailer, excelling in both physical stores and e-commerce.\n",
        "\n",
        "7. McDonald's (MCD)\n",
        "Global fast-food leader with a large franchise model, known for consistent growth and brand strength.\n",
        "\n",
        "8. Johnson & Johnson (JNJ)\n",
        "A diversified healthcare company specializing in medical devices, pharmaceuticals, and consumer health products.\n",
        "\n",
        "9. Alphabet (GOOG)\n",
        "Parent of Google, dominating digital advertising and online services, while investing in AI and innovation.\n",
        "\n",
        "10. PepsiCo (PEP)\n",
        "A multinational food and beverage company with popular brands like Pepsi, Gatorade, and Frito-Lay, ensuring steady growth.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw85_4CSawZG"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKurkeCIw22l"
      },
      "source": [
        "Here we are importing the dataset for Scott Franklin through excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_duKuwDPlqno",
        "outputId": "d460007a-f930-4414-ad4d-b8864065079e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "excel_path = \"/content/Scott Franklin Final.xlsx\"\n",
        "xls = pd.ExcelFile(excel_path)\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for sheet_name in xls.sheet_names:\n",
        "    # Parse the sheet and strip extra whitespace from column names\n",
        "    df = xls.parse(sheet_name)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Convert the Date column to datetime and set it as the index\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], infer_datetime_format=True, errors=\"coerce\")\n",
        "    df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "    # Use the sheet name as the ticker identifier\n",
        "    ticker = sheet_name.strip()\n",
        "\n",
        "    # Define the ticker-specific column names (these should match exactly your Excel headers)\n",
        "    open_col      = f\"Open_{ticker}\"\n",
        "    high_col      = f\"High_{ticker}\"\n",
        "    low_col       = f\"Low_{ticker}\"\n",
        "    close_col     = f\"Close_{ticker}\"\n",
        "    adj_close_col = f\"Adj Close_{ticker}\"\n",
        "    volume_col    = f\"Volume_{ticker}\"\n",
        "    buy_col       = f\"Buy_{ticker}\"\n",
        "    sell_col      = f\"Sell_{ticker}\"\n",
        "    net_col       = f\"Holding_{ticker}\"\n",
        "\n",
        "    # Clean numeric columns: remove commas and convert to float for Buy, Sell, and Net columns\n",
        "    for col in [buy_col, sell_col, net_col]:\n",
        "        if col in df.columns:\n",
        "            df.loc[:, col] = pd.to_numeric(\n",
        "                df[col].replace({',': ''}, regex=True), errors=\"coerce\"\n",
        "            ).fillna(0)\n",
        "\n",
        "    # --- Calculate Trade Metrics for this ticker ---\n",
        "    # Create a trade action column specific for this ticker\n",
        "    trade_action_col = f\"Trade_Action_{ticker}\"\n",
        "    def determine_trade_action(row):\n",
        "        # If both Buy and Sell are > 0, return \"Buy & Sell\"\n",
        "        if row[buy_col] > 0 and row[sell_col] > 0:\n",
        "            return \"Buy & Sell\"\n",
        "        elif row[buy_col] > 0:\n",
        "            return \"Buy\"\n",
        "        elif row[sell_col] > 0:\n",
        "            return \"Sell\"\n",
        "        else:\n",
        "            return \"No Trade\"\n",
        "    df[trade_action_col] = df.apply(determine_trade_action, axis=1)\n",
        "\n",
        "    # Calculate next-day return based on the ticker-specific Close column\n",
        "    next_day_return_col = f\"Next_Day_Return_{ticker}\"\n",
        "    df[next_day_return_col] = df[close_col].pct_change().shift(-1)\n",
        "\n",
        "    # Evaluate trade impact for this ticker\n",
        "    trade_impact_col = f\"Trade_Impact_{ticker}\"\n",
        "    def evaluate_trade(row):\n",
        "        action = row[trade_action_col]\n",
        "        ret = row[next_day_return_col]\n",
        "        if pd.isnull(ret):\n",
        "            return \"No Data\"\n",
        "        if action == \"Buy\":\n",
        "            return \"Good Trade\" if ret > 0 else \"Bad Trade\"\n",
        "        elif action == \"Sell\":\n",
        "            return \"Good Trade\" if ret < 0 else \"Bad Trade\"\n",
        "        elif action == \"Buy & Sell\":\n",
        "            return \"No Difference\"\n",
        "        else:\n",
        "            return \"No Trade\"\n",
        "    df[trade_impact_col] = df.apply(evaluate_trade, axis=1)\n",
        "\n",
        "\n",
        "    # Calculate net change in the net holding (Buy Sell) for this ticker\n",
        "    net_change_col = f\"Net_Change_{ticker}\"\n",
        "    df[net_change_col] = df[net_col].shift(1) - df[net_col]\n",
        "\n",
        "    # NEW: Relate portfolio holdings to next-day return.\n",
        "    # Multiply the net holding (Buy Sell) by the next-day return to get the dollar impact.\n",
        "    portfolio_impact_col = f\"Portfolio_Impact_{ticker}\"\n",
        "    df[portfolio_impact_col] = df[net_col] * df[next_day_return_col]\n",
        "\n",
        "    # Append the processed DataFrame to our list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all processed sheets side by side (wide) on the Date index\n",
        "df_sf = pd.concat(df_list, axis=1, join=\"outer\")\n",
        "df_sf.sort_index(inplace=True)\n",
        "\n",
        "# Display final column list and a preview of the wide DataFrame\n",
        "print(\"Final columns:\")\n",
        "print(df_sf.columns.tolist())\n",
        "print(df_sf.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EiZtpL7X9AI0",
        "outputId": "2634ca6b-bf42-4b40-afac-3c713999d18d"
      },
      "outputs": [],
      "source": [
        "df_sf.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p77xgIGktyl"
      },
      "source": [
        "Metrics:\n",
        "\n",
        "Date, Open, High, Low, Close, Adj Close, Volume were imported from Yahoo Finance starting from 01/02/2024 to 02/24/2025.\n",
        "\n",
        "Added Metrics:\n",
        "\n",
        "1. Buy = Did they buy on that day?\n",
        "\n",
        "2. Sell = Did they sell on that day?\n",
        "\n",
        "  - Imported their buy and sell trade data from Quiver Quantitative.\n",
        "\n",
        "  - We were given ranges of how much they traded and decided used the midpoint number to make sure that it would be the most accurate in this case.\n",
        "\n",
        "3. Holding = How much do they hold in that certain stock on that certain day?\n",
        "\n",
        "  - Imported how much they money held in each stock from Quiver Quantitative\n",
        "\n",
        "4. Trade Action = Was the trade a buy or sell? Or no trade?\n",
        "\n",
        "5. Next Day Return =  Calculates the net day return based on the closed price of that stock from the previous day\n",
        "\n",
        "6. Trade Impact = If they made a trade, was it “Good” or “Bad” the next day? “No Trade” → not evaluated.\n",
        "\n",
        "7. Net Change = Calculates the net change of their hodldings in that stock from the previous day to the current day\n",
        "\n",
        "8. Portfolio Impact = How much did their overall holding's value change because of price movement, whether you traded or not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ke2HtHlkxQh"
      },
      "source": [
        "These metrics let us see the whole picture of how a portfolio is performing. They show us how much of the change comes from the market itself and how much comes from the investor's own trading decisions. This helps us understand the underlying reasons for gains or losses in the portfolio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HdNia6TfcIV"
      },
      "source": [
        "#### Shape of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTwsW0OBfak-",
        "outputId": "025dfb76-7c66-4254-94f7-912ab94d4986"
      },
      "outputs": [],
      "source": [
        "df_sf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yi49fcRxh25"
      },
      "source": [
        "There are 287 rows and 140 columns in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd2QIOp5a4HY"
      },
      "source": [
        "### .info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29jZJfTWbAKz",
        "outputId": "52957b53-3a49-416f-bc0d-0a174b5f0cbc"
      },
      "outputs": [],
      "source": [
        "df_sf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF0BzsiYbpTF"
      },
      "source": [
        "- The dataframe has 287 rows (indexed by dates from 2024-01-02 to 2025-02-24)\n",
        "- There are 140 columns, with names running from “Open_JPM” through “Portfolio_Impact_PEP.”\n",
        "- Of the 140 columns, 102 are floating-point, 18 are integers, and 20 are objects (often strings or mixed data).\n",
        "- The entire DataFrame is using about 316 KB of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9p7hyZa6I9"
      },
      "source": [
        "### Data Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeTeKzV4bpE0",
        "outputId": "78621d95-ea1f-4b6d-c710-eb5256ab154e"
      },
      "outputs": [],
      "source": [
        "# Increase the max rows (or columns) displayed in the console\n",
        "pd.set_option('display.max_rows', None)  # No limit on rows\n",
        "# pd.set_option('display.max_columns', None)  # No limit on columns if needed\n",
        "\n",
        "# Now printing dtypes will not truncate\n",
        "print(\"Data types for df_sf columns:\")\n",
        "print(df_sf.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCERT6LIcRJv"
      },
      "outputs": [],
      "source": [
        "for col in df_sf.columns:\n",
        "    if \"Trade_Action\" in col:\n",
        "        df_sf[col] = df_sf[col].astype('category')\n",
        "    elif \"Trade_Impact\" in col:\n",
        "        df_sf[col] = df_sf[col].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iuxJokfxxbC"
      },
      "source": [
        "Changing the data type for Trade_Action_{ticker} and Trade_Impact{ticker) to category instead of object to help with analysis and uses less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIQ01x1XcUo5",
        "outputId": "e6c9a4fb-8628-43b9-d4fd-96fa4cb5165a"
      },
      "outputs": [],
      "source": [
        "# Increase the max rows (or columns) displayed in the console\n",
        "pd.set_option('display.max_rows', None)  # No limit on rows\n",
        "# pd.set_option('display.max_columns', None)  # No limit on columns if needed\n",
        "\n",
        "# Now printing dtypes will not truncate\n",
        "print(\"Data types for df_sf columns:\")\n",
        "print(df_sf.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53x9UInlx84y"
      },
      "source": [
        "Checking to see if the changes were made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU2Qr6Bsa8KZ"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26_lJY-kc2Ej",
        "outputId": "7df967f7-0783-441b-84dc-ef86736e43cc"
      },
      "outputs": [],
      "source": [
        "df_sf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ds8NWrNrcas-",
        "outputId": "b6872427-ae98-4b0b-c9dc-0f85b5138ed3"
      },
      "outputs": [],
      "source": [
        "# Drop rows with any missing values\n",
        "df_sf.dropna(inplace=True)\n",
        "\n",
        "# Display the DataFrame after removing missing values\n",
        "print(df_sf.head(10))\n",
        "df_sf.info()\n",
        "df_sf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGmwJFeyfBhx",
        "outputId": "63bc71f5-9483-49dc-efac-ed391c755f88"
      },
      "outputs": [],
      "source": [
        "df_sf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGUQkJHUyECm"
      },
      "source": [
        "Checking for missing values. The missing values are coming from Next_Day_Return_{ticker} which calculates the net day return based on the closed price of that stock from the previous day and since the data starts from 2024-01-02 and there is no data on 2024-01-01 therefore it shows up NaN. Additionally, in this dataset AAPL and HD have 37 less rows than the other 8 stocks which is why it is showing 37 missing values for both AAPL and HD columns. We have decided to delete them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpNv6iYw9Gso"
      },
      "source": [
        "##Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FeOlcMX9KSj"
      },
      "source": [
        "###Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWh8EjixDsI9"
      },
      "source": [
        "#### JPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "D_ru9veWDte9",
        "outputId": "2549f82a-b891-4ccc-e19d-e0ed3b608217"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JPM features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jpm_columns = [f\"{feature}_JPM\" for feature in features]\n",
        "\n",
        "# Filter dataset for JPM, dropping rows with NaNs\n",
        "df_jpm = df_sf[jpm_columns].dropna()\n",
        "\n",
        "# Set up color palette for JPM\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JPM\"\n",
        "    if col_name in df_jpm.columns:\n",
        "        sns.histplot(data=df_jpm, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of JPM {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_JPM\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zWGVdagYILt"
      },
      "source": [
        "If we look at price metrics (Open, High, Low, Close, and Adjusted Close), they show a right-skewed distribution, with most trading activity around 190–220. Trading volume follows a similar pattern, with moderate daily activity but significant spikes, which is probably because of institutional trades or major news impact.\n",
        "\n",
        "Trading volume follows a right-skewed distribution too, meaning most days saw moderate volume, but there were occasional high-volume spikes.\n",
        "\n",
        "The buy and sell activity histograms reveal that there was no buying activity, whereas large sell-offs occasionally occurred.\n",
        "\n",
        "In terms of returns and portfolio impact, the next-day return histogram follows a normal distribution, which is expected in an efficient market. However, net change and portfolio impact show occasional extreme values, indicating that specific trades had a notable effect on portfolio performance.\n",
        "\n",
        "Overall, JPM's stock data reflects consistent trading patterns with periods of volatility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pflKyYfJDtyo"
      },
      "source": [
        "#### AAPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "vkvQvsGKDvdx",
        "outputId": "692f4931-8257-4856-aeb4-ceff5dbecfc1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AAPL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "aapl_columns = [f\"{feature}_AAPL\" for feature in features]\n",
        "\n",
        "# Filter dataset for AAPL, dropping rows with NaNs\n",
        "df_aapl = df_sf[aapl_columns].dropna()\n",
        "\n",
        "# Set up color palette for AAPL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AAPL\"\n",
        "    if col_name in df_aapl.columns:\n",
        "        sns.histplot(data=df_aapl, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of AAPL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_AAPL\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMHGLzMHYdSp"
      },
      "source": [
        "The price distributions for AAPL (Open, High, Low, Close, and Adjusted Close) indicate that most trading activity occurred around $200–$240, with occasional spikes.\n",
        "\n",
        "Trading volume follows a right-skewed pattern, meaning lower volumes were more common, but there were occasional large spikes, likely driven by market events. In this time period there is no buy activity, while there are some periodic large sell-offs.\n",
        "\n",
        "Next-day returns follow a normal distribution, centering around zero, which aligns with expectations in an efficient market. Net change and portfolio impact show occasional extreme values, meaning specific trades had a notable effect on portfolio fluctuations.\n",
        "\n",
        "Overall, AAPL stock shows stable yet volatile trading trends, with periods of heightened activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW1b1gIbDvrL"
      },
      "source": [
        "#### HD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "627c7G-WDxEc",
        "outputId": "0608e44e-4371-4113-9bbc-7ffc54f7d86f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant HD features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "hd_columns = [f\"{feature}_HD\" for feature in features]\n",
        "\n",
        "# Filter dataset for HD, dropping rows with NaNs\n",
        "df_hd = df_sf[hd_columns].dropna()\n",
        "\n",
        "# Set up color palette for HD\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_HD\"\n",
        "    if col_name in df_hd.columns:\n",
        "        sns.histplot(data=df_hd, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of HD {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_HD\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8VoWjGUYeuV"
      },
      "source": [
        "The price distributions (Open, High, Low, Close, and Adjusted Close) for Home Depot (HD) show a relatively even spread between 340 and 420, with no sharp concentration in any particular range. The KDE lines suggest a stable price movement with some fluctuations.\n",
        "\n",
        "Trading volume follows a right-skewed pattern, meaning most days saw moderate trading activity, but a few high-volume spikes occurred, likely due to external market factors. The Buy and Sell activity histograms are empty, indicating no recorded transactions in these categories. Holdings remain constant, suggesting a long-term investment strategy without frequent trading.\n",
        "\n",
        "Next-day returns follow a normal distribution, meaning small daily fluctuations in price are common. Portfolio impact shows a centered distribution around zero, with some days having notable positive or negative effects on the portfolio.\n",
        "\n",
        "Overall, HD stock appears relatively stable, with consistent trading volume and limited active buying or selling activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3rbmCjrDxXk"
      },
      "source": [
        "#### MSFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "eLRg8mepDyyW",
        "outputId": "12897bfa-db55-4eb1-f005-4773ad62ab5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MSFT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "msft_columns = [f\"{feature}_MSFT\" for feature in features]\n",
        "\n",
        "# Filter dataset for MSFT, dropping rows with NaNs\n",
        "df_msft = df_sf[msft_columns].dropna()\n",
        "\n",
        "# Set up color palette for MSFT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MSFT\"\n",
        "    if col_name in df_msft.columns:\n",
        "        sns.histplot(data=df_msft, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of MSFT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_MSFT\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nwtxT2vYhnY"
      },
      "source": [
        "Microsoft (MSFT) has a fairly normal price distribution, with most trading happening between 400 and 440 across Open, High, Low, and Close prices. There’s a slight skew towards higher prices, but overall, the price movements appear stable without major spikes.\n",
        "\n",
        "Trading volume is right-skewed, meaning most days saw moderate activity, but there were some big trading days. Buy activity is non-existent, while Sell activity is minimal but occasionally spikes. The holdings data shows two distinct levels, indicating potential portfolio rebalancing at set intervals.\n",
        "\n",
        "Next-day returns follow a normal distribution, suggesting that MSFT’s daily price changes are mostly small and predictable. The portfolio impact distribution is centered around zero, though some outlier days had a significant effect on the overall portfolio value.\n",
        "\n",
        "Overall, MSFT looks like a stable, steadily traded stock in this portfolio, with occasional volume spikes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4sql3WwDzE7"
      },
      "source": [
        "#### V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "w4rFj8rTD0UX",
        "outputId": "7166fb25-26ef-4cfc-a42c-d7f9f4b8c508"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant V features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "v_columns = [f\"{feature}_V\" for feature in features]\n",
        "\n",
        "# Filter dataset for V, dropping rows with NaNs\n",
        "df_v = df_sf[v_columns].dropna()\n",
        "\n",
        "# Set up color palette for V\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_V\"\n",
        "    if col_name in df_v.columns:\n",
        "        sns.histplot(data=df_v, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of V {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_V\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pky5PLemYjun"
      },
      "source": [
        "Visa's (V) stock analysis reveals some interesting patterns. The histograms for its open, high, and low prices show a strong concentration around the 270–290 range, with some outliers in the higher 300s. This suggests that for most of the analyzed period, Visa’s stock traded within a relatively narrow range, with occasional spikes.\n",
        "\n",
        "The volume histogram is really skewed, with most trading days seeing lower volume and a few days experiencing significantly higher spikes.\n",
        "\n",
        "The buy, sell, and holding histograms show that Visa was primarily held with little change in positions, which could indicate confidence in long-term value rather than active trading.\n",
        "\n",
        "Overall, Visa’s stock behavior in this analysis suggests stability, with occasional movements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV-A7E1zD0ic"
      },
      "source": [
        "#### WMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "sRjn4awzD2A5",
        "outputId": "41a8238a-c30c-47d6-81e2-c05d0da01279"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant WMT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "wmt_columns = [f\"{feature}_WMT\" for feature in features]\n",
        "\n",
        "# Filter dataset for JPM, dropping rows with NaNs\n",
        "df_wmt = df_sf[wmt_columns].dropna()\n",
        "\n",
        "# Set up color palette for WMT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_WMT\"\n",
        "    if col_name in df_wmt.columns:\n",
        "        sns.histplot(data=df_wmt, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of WMT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_WMT\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMc_gYvGYmd2"
      },
      "source": [
        "The Walmart (WMT) histograms show that most of its opening, high, and low prices are clustered in the lower price range, around 60-80, with a few outliers above 90.\n",
        "\n",
        "The closing and adjusted close prices follow a similar pattern, suggesting that the stock has mostly traded in a stable range over time. The volume histogram is heavily skewed, meaning there were more days with lower trading volume, but some days saw significant spikes.\n",
        "\n",
        "The next-day return distribution is centered around zero, meaning small daily fluctuations. The net change and portfolio impact histograms also show a roughly normal distribution, indicating most changes were minor, with a few larger shifts. Overall, WMT stock seems relatively stable, with occasional price swings but no extreme volatility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XWU6-3eD2nj"
      },
      "source": [
        "#### MCD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "-hd31XeMD4IW",
        "outputId": "43a30729-c4c9-4efe-b0cf-292ee20f7b4d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MCD features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "mcd_columns = [f\"{feature}_MCD\" for feature in features]\n",
        "\n",
        "# Filter dataset for MCD, dropping rows with NaNs\n",
        "df_mcd = df_sf[mcd_columns].dropna()\n",
        "\n",
        "# Set up color palette for MCD\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MCD\"\n",
        "    if col_name in df_mcd.columns:\n",
        "        sns.histplot(data=df_mcd, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of MCD {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_MCD\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlsw1UFYoy3"
      },
      "source": [
        "Looking at the MCD stock data, we can see that its opening, high, and low prices are mostly clustered around the 260-300 range, with some occasional outliers. The distributions are slightly skewed, suggesting some volatility.\n",
        "\n",
        "The closing and adjusted closing prices follow a similar trend, reinforcing that MCD stock generally fluctuates within this range. The trading volume histogram is heavily right-skewed, meaning most days have relatively low trading volumes, but there are occasional spikes.\n",
        "\n",
        "The next-day return distribution is centered around 0, meaning daily returns are fairly balanced with no extreme trends.\n",
        "\n",
        "The portfolio impact histogram is close to a normal distribution, meaning small gains and losses are more frequent, with fewer extreme changes.\n",
        "\n",
        "Overall, MCD’s stock movement seems fairly steady, with occasional volume spikes and moderate daily fluctuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6sdGA4vD4Sh"
      },
      "source": [
        "#### JNJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "dCM1jRyiD5p7",
        "outputId": "9bb19a92-7c05-4836-a30e-35f8f9617dec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JNJ features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jnj_columns = [f\"{feature}_JNJ\" for feature in features]\n",
        "\n",
        "# Filter dataset for JNJ, dropping rows with NaNs\n",
        "df_jnj = df_sf[jnj_columns].dropna()\n",
        "\n",
        "# Set up color palette for JNJ\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JNJ\"\n",
        "    if col_name in df_jnj.columns:\n",
        "        sns.histplot(data=df_jnj, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of JNJ {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_JNJ\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmwX0TtaYqwp"
      },
      "source": [
        "The JNJ stock data shows a spread in opening, high, and low prices, with a slight skew. The closing price follows a similar pattern. Trading volume is right-skewed, meaning most days had lower trading volumes with a few high-volume outliers. There is no buy but we can see there is a sell. Portfolio impact looks normally distributed, suggesting balanced fluctuations. The net change has an extreme outlier, indicating a big shift on certain days. Overall, JNJ stock data seems relatively stable, but with occasional spikes in activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7v_Lf-9D56W"
      },
      "source": [
        "#### GOOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "Xrp4-LgbD7Nu",
        "outputId": "737a0c02-7cd4-4007-afdd-959425ad9d56"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GOOG features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "goog_columns = [f\"{feature}_GOOG\" for feature in features]\n",
        "\n",
        "# Filter dataset for GOOG, dropping rows with NaNs\n",
        "df_goog = df_sf[goog_columns].dropna()\n",
        "\n",
        "# Set up color palette for GOOG\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GOOG\"\n",
        "    if col_name in df_goog.columns:\n",
        "        sns.histplot(data=df_goog, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of GOOG {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_GOOG\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO_balo9YtQw"
      },
      "source": [
        "GOOG’s price distributions have most trading activity around 160–180. Trading volume is right-skewed, showing occasional high-volume days, likely due to market events. Buy activity is absent, while Sell activity is minimal, suggesting low trading frequency in this portfolio. Holdings show a bimodal pattern, indicating periodic adjustments. Next-Day Returns follow a normal distribution, and portfolio impact is mostly neutral, though some extreme changes suggest occasional significant influence. Overall, GOOG stock appears stable with some rare high-volume movements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXMztYwyD7g-"
      },
      "source": [
        "#### PEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "4YRua29NQMEI",
        "outputId": "066619c0-2e49-4c4e-8924-be1ffa61a759"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant PEP features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "pep_columns = [f\"{feature}_PEP\" for feature in features]\n",
        "\n",
        "# Filter dataset for PEP, dropping rows with NaNs\n",
        "df_pep = df_sf[pep_columns].dropna()\n",
        "\n",
        "# Set up color palette for PEP\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 4, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate histograms\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_PEP\"\n",
        "    if col_name in df_pep.columns:\n",
        "        sns.histplot(data=df_pep, x=col_name, ax=axes[col_idx],\n",
        "                     color=feature_colors[feature], kde=True)\n",
        "        axes[col_idx].set_title(f\" Histogram of PEP {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_xlabel(f\"{feature}_PEP\", fontsize=10)\n",
        "        axes[col_idx].set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW7X1ee4YvLq"
      },
      "source": [
        "PEP’s price distributions show most trading occurring between 160–175. Trading volume is right-skewed, indicating that while most days had moderate activity, occasional spikes occurred. Buy activity is absent, and Sell activity is minimal, suggesting limited trades. Holdings exhibit a bimodal distribution, possibly due to periodic portfolio adjustments. Next-Day Returns follow a normal distribution, while portfolio impact is mostly neutral, though occasional shifts suggest some high-impact trading days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG5zdK5pYy0o"
      },
      "source": [
        "## Scott Franklin's Portfolio Strategy & Analysis\n",
        "\n",
        "1. Diversification Across Sectors\n",
        "\n",
        "  - The portfolio is spread across six different sectors, reducing sector-specific risks:\n",
        "\n",
        "    - Technology: Apple (AAPL), Microsoft (MSFT)\n",
        "    - Financials: JPMorgan Chase (JPM), Visa (V)\n",
        "    - Consumer Staples: Walmart (WMT), PepsiCo (PEP)\n",
        "    - Consumer Discretionary: Home Depot (HD), McDonald's (MCD)\n",
        "    - Health Care: Johnson & Johnson (JNJ)\n",
        "    - Communication Services: Alphabet (GOOG)\n",
        "\n",
        "  - This mix balances high-growth tech stocks with stable blue-chip companies, creating a well-rounded investment strategy.\n",
        "\n",
        "2. Heavy Emphasis on Large-Cap, Blue-Chip Stocks\n",
        "\n",
        "  - All 10 companies are mega-cap stocks, each with a market capitalization of over $200 billion.\n",
        "\n",
        "  - The presence of defensive stocks like JNJ, PEP, and WMT suggests a focus on stability, while tech giants AAPL, MSFT, and GOOG provide high-growth opportunities.\n",
        "\n",
        "3. Connection to Scott Franklin’s Investment Strategy\n",
        "\n",
        "  - The portfolio suggests an institutional-style investment approach focused on:\n",
        "\n",
        "  - Low-risk, high-reward investments in established companies rather than speculative or small-cap stocks.\n",
        "\n",
        "4. Potential Strategy Insights\n",
        "\n",
        "  - The portfolio is low-volatility, with stocks that historically weather economic downturns well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEpky8C39cgd"
      },
      "source": [
        "###Boxplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U04OoHrK9-ui"
      },
      "source": [
        "####JPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "YlkSzot29OhV",
        "outputId": "333a0e5e-5677-467d-a234-252efd97d8ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JPM features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jpm_columns = [f\"{feature}_JPM\" for feature in features]\n",
        "\n",
        "# Filter dataset for JPM\n",
        "df_jpm = df_sf[jpm_columns].dropna()\n",
        "\n",
        "# Set up color palette for JPM\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JPM\"\n",
        "    if col_name in df_jpm.columns:\n",
        "        sns.boxplot(y=df_jpm[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"JPM {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWNEoQnY7jQ"
      },
      "source": [
        "The boxplots for JPM show that its stock prices (Open, High, Low, Close, and Adj Close) are fairly spread out, with a median around 200-220 and some outliers. Volume has a right-skewed distribution, with occasional high trading spikes. There’s almost no Buy activity and minimal Sell activity, with a few extreme sell outliers. Holdings remain stable, suggesting a long-term position. Next-Day Returns are mostly centered around 0%, but some outliers show occasional volatility. Net Change and Portfolio Impact have some big outliers, indicating that JPM sometimes has significant portfolio effects. Overall, JPM looks like a stable holding with occasional large market movements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHwkohMq_N3v"
      },
      "source": [
        "####AAPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "opWSU3oe-f1G",
        "outputId": "64f0c408-c4f7-4115-c5af-39c7fb583f42"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant AAPL features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "aapl_columns = [f\"{feature}_AAPL\" for feature in features]\n",
        "\n",
        "# Filter dataset for MSFT\n",
        "df_aapl = df_sf[aapl_columns].dropna()\n",
        "\n",
        "# Set up color palette for AAPL\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_AAPL\"\n",
        "    if col_name in df_aapl.columns:\n",
        "        sns.boxplot(y=df_aapl[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"AAPL {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tpcx5TnY99R"
      },
      "source": [
        "AAPL's stock prices are stable, with a median around 220 and some price variation. Volume is right-skewed, showing occasional trading spikes. No Buy activity, and minimal Sell activity with some large sell outliers. Holdings are steady, suggesting a long-term position. Next-Day Returns are centered around 0%, but outliers indicate occasional volatility. Net Change and Portfolio Impact show some large movements, but overall, AAPL seems to be a stable core holding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he60n1Ps_r2e"
      },
      "source": [
        "####HD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "wgDpSqc7_n5e",
        "outputId": "cb75a3bf-a5f2-46eb-dc1e-6e522f6b1a99"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant HD features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "hd_columns = [f\"{feature}_HD\" for feature in features]\n",
        "\n",
        "# Filter dataset for HD\n",
        "df_hd = df_sf[hd_columns].dropna()\n",
        "\n",
        "# Set up color palette for HD\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_HD\"\n",
        "    if col_name in df_hd.columns:\n",
        "        sns.boxplot(y=df_hd[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"HD {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FuyRK3_Y_qa"
      },
      "source": [
        "HD's stock price is stable, with a median around 380-400. Trading volume has occasional spikes, but overall activity is moderate. No Buy or Sell activity, indicating a long-term hold strategy. Holdings remain steady, suggesting confidence in the stock. Next-Day Returns are small, with limited volatility. Portfolio Impact is mostly neutral, with occasional fluctuations. HD appears to be a low-risk, steady investment in the portfolio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdlaMrHbAKQV"
      },
      "source": [
        "####MSFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "yGjXt0mrAIrN",
        "outputId": "380530e9-52a2-4d08-cca2-f153b509bbfb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MSFT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "msft_columns = [f\"{feature}_MSFT\" for feature in features]\n",
        "\n",
        "# Filter dataset for MSFT\n",
        "df_msft = df_sf[msft_columns].dropna()\n",
        "\n",
        "# Set up color palette for MSFT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MSFT\"\n",
        "    if col_name in df_msft.columns:\n",
        "        sns.boxplot(y=df_msft[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"MSFT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pypIYXz8ZEmv"
      },
      "source": [
        "MSFT's stock price ranges around 410-430, with some outliers indicating occasional volatility. Trading volume has spikes, suggesting increased activity during specific events. No Buy activity and minimal Sell activity, implying a long-term holding strategy. Holdings remain consistent, reflecting confidence in the stock. Next-Day Returns show slight variations, but overall, MSFT remains a stable investment with occasional high-impact movements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I75wadUnASAJ"
      },
      "source": [
        "####V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "cD1FsSyeANVb",
        "outputId": "52542875-4fad-482f-8659-355f422da6ac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant V features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "v_columns = [f\"{feature}_V\" for feature in features]\n",
        "\n",
        "# Filter dataset for V\n",
        "df_v = df_sf[v_columns].dropna()\n",
        "\n",
        "# Set up color palette for V\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_V\"\n",
        "    if col_name in df_v.columns:\n",
        "        sns.boxplot(y=df_v[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"V {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOukT1NZJB-"
      },
      "source": [
        "Visa's stock price ranges around 280-300, showing moderate fluctuations. Volume has occasional spikes, suggesting increased activity on some days. No Buy or Sell activity, indicating a long-term hold strategy. Holdings remain steady, showing confidence in the stock. Next-Day Returns are mostly stable, with some outliers suggesting occasional volatility, but overall, Visa appears to be a consistent investment with limited risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D14MFg1qAve3"
      },
      "source": [
        "####WMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "hafgoZkPAnYS",
        "outputId": "dac155dc-8b0b-4ed3-8efe-1540fce32fcf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant WMT features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "wmt_columns = [f\"{feature}_WMT\" for feature in features]\n",
        "\n",
        "# Filter dataset for WMT\n",
        "df_wmt = df_sf[wmt_columns].dropna()\n",
        "\n",
        "# Set up color palette for WMT\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_WMT\"\n",
        "    if col_name in df_wmt.columns:\n",
        "        sns.boxplot(y=df_wmt[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"WMT {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDV1k3IVZsze"
      },
      "source": [
        "Walmart's stock price fluctuates between 70-90, showing moderate stability. Trading volume has occasional spikes, suggesting periods of increased market activity. No Buy activity and minimal Sell activity, reinforcing a long-term holding strategy. Holdings remain consistent, showing confidence in the stock. Next-Day Returns are mostly stable, with some outliers indicating occasional volatility. Overall, Walmart appears to be a steady investment with controlled risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9w_CgwBJIz"
      },
      "source": [
        "####MCD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "3hSFvfVYBHGd",
        "outputId": "2e3ddc74-b3a1-4377-c4ca-53f19e8b7291"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant MCD features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "mcd_columns = [f\"{feature}_MCD\" for feature in features]\n",
        "\n",
        "# Filter dataset for MCD\n",
        "df_mcd = df_sf[mcd_columns].dropna()\n",
        "\n",
        "# Set up color palette for MCD\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_MCD\"\n",
        "    if col_name in df_mcd.columns:\n",
        "        sns.boxplot(y=df_mcd[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"MCD {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRwF7ugRZqRs"
      },
      "source": [
        "McDonald's stock price ranges between 270-300, showing moderate stability. Trading volume has occasional spikes, indicating periods of higher market interest. No Buy activity and minimal Sell activity, suggesting a long-term holding approach. Holdings remain stable, reinforcing confidence in the stock. Next-Day Returns are mostly consistent, with a few outliers indicating short-term volatility. Overall, McDonald's appears to be a steady investment with controlled risk and occasional market fluctuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezvRy__aBkji"
      },
      "source": [
        "####JNJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "DBfYcR6ZBh6x",
        "outputId": "9ddf3e1f-5b40-4bcf-fc8e-9ebcc1004360"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant JNJ features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "jnj_columns = [f\"{feature}_JNJ\" for feature in features]\n",
        "\n",
        "# Filter dataset for JNJ\n",
        "df_jnj = df_sf[jnj_columns].dropna()\n",
        "\n",
        "# Set up color palette for JNJ\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_JNJ\"\n",
        "    if col_name in df_jnj.columns:\n",
        "        sns.boxplot(y=df_jnj[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"JNJ {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD76qDpGZn0O"
      },
      "source": [
        "Johnson & Johnson's stock price fluctuates between 145-165, showing consistent stability. Trading volume spikes occasionally, likely due to major market events. Minimal Sell activity and no Buy transactions, indicating a long-term holding approach. Holdings remain stable, suggesting strong confidence in the stock. Next-Day Returns show low volatility, though a few outliers suggest occasional fluctuations. Overall, JNJ appears to be a steady, low-risk investment with limited short-term trading activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIMqfa3OB6lY"
      },
      "source": [
        "####GOOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "HPEnJRkaB59h",
        "outputId": "23dea375-b311-4d30-afc2-0ad54d7ea412"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant GOOG features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "goog_columns = [f\"{feature}_GOOG\" for feature in features]\n",
        "\n",
        "# Filter dataset for GOOG\n",
        "df_goog = df_sf[goog_columns].dropna()\n",
        "\n",
        "# Set up color palette for GOOG\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_GOOG\"\n",
        "    if col_name in df_goog.columns:\n",
        "        sns.boxplot(y=df_goog[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"GOOG {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXWqq1FZibr"
      },
      "source": [
        "Google's (GOOG) stock price ranges between 140-200, with moderate volatility. Trading volume has frequent spikes, indicating occasional market interest. Minimal Sell activity and no Buy transactions, suggesting a long-term holding strategy. Holdings remain stable, reinforcing confidence in the stock. Next-Day Returns show a normal distribution, with a few outliers hinting at occasional price jumps. Overall, GOOG appears to be a steadily held investment with periodic trading volume surges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRWplWNCOAp"
      },
      "source": [
        "####PEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "199u-HWrCMV3",
        "outputId": "dd631b58-7f93-4ef0-e775-e375d7468147"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define relevant PEP features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Format column names for consistency\n",
        "pep_columns = [f\"{feature}_PEP\" for feature in features]\n",
        "\n",
        "# Filter dataset for PEP\n",
        "df_pep = df_sf[pep_columns].dropna()\n",
        "\n",
        "# Set up color palette for PEP\n",
        "palette = sns.color_palette(\"husl\", len(features))\n",
        "feature_colors = {feature: palette[i] for i, feature in enumerate(features)}\n",
        "\n",
        "# Set up grid layout: 1 row, multiple columns (one for each feature)\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(len(features) * 3.5, 5), sharex=False)\n",
        "\n",
        "# Iterate over features to generate separate boxplots\n",
        "for col_idx, feature in enumerate(features):\n",
        "    col_name = f\"{feature}_PEP\"\n",
        "    if col_name in df_pep.columns:\n",
        "        sns.boxplot(y=df_pep[col_name], ax=axes[col_idx], color=feature_colors[feature])\n",
        "        axes[col_idx].set_title(f\"PEP {feature}\", fontsize=12, fontweight='bold')\n",
        "        axes[col_idx].set_ylabel(\"Value\", fontsize=10)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL2dKqtJZjJR"
      },
      "source": [
        "PepsiCo (PEP) trades between 145-180, showing moderate price fluctuations. Trading volume has occasional spikes, likely due to market events. Minimal Sell activity and no Buy transactions, suggesting a long-term holding strategy. Holdings remain stable, indicating confidence in the stock. Next-Day Returns are normally distributed, with a few outliers suggesting occasional price swings. Overall, PEP appears to be a steady investment with low trading activity and periodic volume surges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxcM68SNae_q"
      },
      "source": [
        "Scott Franklin’s Investment Strategy Based on Boxplot Analysis\n",
        "General Strategy:\n",
        "\n",
        "- Long-Term Holding Approach: Minimal buy/sell activity suggests low trading frequency, indicating a buy-and-hold strategy rather than active trading.\n",
        "\n",
        "- Risk Control & Consistency: Stocks show moderate volatility with controlled downside risks, avoiding extreme price swings.\n",
        "\n",
        "Boxplot Insights Across Stocks:\n",
        "\n",
        "- Price Movements: Most stocks trade within a defined range with minimal outliers, showing stability and predictable price action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gJRgg_tDFpK"
      },
      "source": [
        "##Bivariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OERSGYfHDJxr"
      },
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VrFeXP-9E3-S",
        "outputId": "57f58284-65ea-4583-be1f-1ba56a396e3b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract tickers from column names while excluding non-ticker suffixes\n",
        "tickers = {col.split('_')[-1] for col in df_sf.columns\n",
        "           if '_' in col and col.split('_')[-1] not in ['Shares', 'Held']}\n",
        "\n",
        "# Define relevant features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Number of tickers\n",
        "num_tickers = len(tickers)\n",
        "\n",
        "# Create subplots (one per ticker) with a larger size\n",
        "fig, axes = plt.subplots(num_tickers, 1, figsize=(10, num_tickers * 7), squeeze=False)\n",
        "\n",
        "# Loop through tickers to compute and plot correlation matrices\n",
        "for idx, ticker in enumerate(sorted(tickers)):  # Sort tickers alphabetically for consistency\n",
        "    # Select relevant columns for the ticker\n",
        "    ticker_cols = [f\"{feature}_{ticker}\" for feature in features if f\"{feature}_{ticker}\" in df_sf.columns]\n",
        "\n",
        "    # Extract data and drop missing values\n",
        "    ticker_data = df_sf[ticker_cols].dropna()\n",
        "\n",
        "    # Ensure there are at least two columns for correlation calculation\n",
        "    if ticker_data.shape[1] < 2:\n",
        "        print(f\"Skipping correlation matrix for {ticker} due to insufficient data.\")\n",
        "        continue\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    ticker_corr = ticker_data.corr()\n",
        "\n",
        "    # Plot heatmap\n",
        "    ax = axes[idx, 0]\n",
        "    sns.heatmap(ticker_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=ax)\n",
        "    ax.set_title(f\"Correlation Matrix for {ticker.upper()}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RExaXOUpv6M-"
      },
      "source": [
        "1. JPM (JP Morgan)\n",
        "- Strong correlation between Buy and Next_Day_Return, suggesting that buying activity is a predictor of next-day performance.\n",
        "- Moderate negative correlation between Sell and Portfolio_Impact, meaning that selling reduces portfolio gains.\n",
        "- Net_Change positively correlated with Adj Close, implying price movements influence adjusted closing prices.\n",
        "\n",
        "2. AAPL (Apple)\n",
        "- High correlation between Open, High, Low, and Close, meaning Apple has stable intraday price movement.\n",
        "- Strong positive correlation between Volume and Net_Change, suggesting increased trading activity drives price swings.\n",
        "- Moderate correlation between Buy and Holding, meaning investors tend to hold positions post-purchase\n",
        "\n",
        "3. HD (Home Depot)\n",
        "- Low correlation between Volume and Close, meaning price is not always affected by trading volume.\n",
        "- Portfolio_Impact strongly correlated with Net_Change, suggesting short-term gains directly influence total portfolio value.\n",
        "- Buy signals weakly correlated with Next_Day_Return, indicating purchases don’t immediately translate into gains.\n",
        "\n",
        "4. MSFT (Microsoft)\n",
        "- High correlation between Close and Adj Close, meaning adjustments for dividends have minimal effect.\n",
        "- Volume and Net_Change moderately correlated, meaning larger trading days coincide with price changes.\n",
        "- Buy and Sell activity moderately correlated, suggesting both bulls and bears actively trade Microsoft.\n",
        "\n",
        "5. V (Visa)\n",
        "- Buy signals highly correlated with Holding, suggesting investors accumulate Visa for long-term growth.\n",
        "- Low correlation between Volume and Price Changes, meaning Visa trades are stable and not heavily influenced by market fluctuations.\n",
        "- Net_Change and Portfolio_Impact highly correlated, indicating Visa contributes significantly to portfolio gains.\n",
        "\n",
        "6. WMT (Walmart)\n",
        "- High correlation between Open and Close, meaning Walmart shows low intraday volatility.\n",
        "- Sell activity negatively correlated with Net_Change, meaning high sell-offs lead to declines.\n",
        "- Volume and Portfolio_Impact weakly correlated, suggesting trading volume does not significantly affect portfolio performance.\n",
        "\n",
        "7. MCD (McDonald's)\n",
        "- Strong correlation between Adj Close and Close, meaning dividends and stock splits have minimal impact.\n",
        "- Moderate correlation between Holding and Next_Day_Return, indicating long-term holders benefit from stable returns.\n",
        "- Low correlation between Buy and Next_Day_Return, suggesting buying activity does not immediately translate into gains.\n",
        "\n",
        "8. JNJ (Johnson & Johnson)\n",
        "- High correlation between Portfolio_Impact and Net_Change, showing JNJ contributes significantly to portfolio returns.\n",
        "- Low correlation between Volume and Close, meaning JNJ is less influenced by daily trading fluctuations.\n",
        "- Sell and Net_Change negatively correlated, suggesting sell-offs reduce stock gains.\n",
        "\n",
        "9. GOOG (Google)\n",
        "- Strong correlation between Volume and Net_Change, suggesting price swings are driven by heavy trading activity.\n",
        "- Buy and Next_Day_Return moderately correlated, meaning purchasing Google stocks can lead to short-term gains.\n",
        "- Negative correlation between Sell and Portfolio_Impact, meaning large sell-offs impact long-term gains.\n",
        "\n",
        "10. PEP (PepsiCo)\n",
        "- High correlation between Holding and Portfolio_Impact, meaning Pepsi’s long-term investors benefit from steady portfolio gains.\n",
        "- Weak correlation between Volume and Price Changes, indicating Pepsi is a low-volatility stock.\n",
        "- Buy and Sell show low correlation, meaning market sentiment doesn’t fluctuate drastically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4L5zJNEOfy"
      },
      "source": [
        "## Pair Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jgfCcQd5EIzK",
        "outputId": "7013587b-8952-4ba5-903b-b86609e2a7a2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract tickers from column names while excluding non-ticker suffixes\n",
        "tickers = {col.split('_')[-1] for col in df_sf.columns # Changed stock_data_sf to df_sf\n",
        "           if '_' in col and col.split('_')[-1] not in ['Shares', 'Held']}\n",
        "\n",
        "# Define relevant features\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"Buy\", \"Sell\", \"Holding\", \"Next_Day_Return\", \"Net_Change\", \"Portfolio_Impact\"]\n",
        "\n",
        "# Loop through tickers and generate pairplots\n",
        "for ticker in sorted(tickers):  # Sort for consistent order\n",
        "    # Select relevant columns for the ticker\n",
        "    ticker_cols = [f\"{feature}_{ticker}\" for feature in features if f\"{feature}_{ticker}\" in df_sf.columns] # Changed stock_data_sf to df_sf\n",
        "\n",
        "    # Extract data and drop missing values\n",
        "    ticker_data = df_sf[ticker_cols].dropna() # Changed stock_data_sf to df_sf\n",
        "\n",
        "    # Ensure there are at least two numeric columns\n",
        "    if ticker_data.shape[1] < 2:\n",
        "        print(f\"Skipping pairplot for {ticker} due to insufficient data.\")\n",
        "        continue\n",
        "\n",
        "    # Rename columns for better readability in pairplot\n",
        "    ticker_data = ticker_data.rename(columns={col: col.replace(f\"_{ticker}\", \"\") for col in ticker_cols})\n",
        "\n",
        "    # Generate pairplot\n",
        "    print(f\"Generating pairplot for {ticker}...\")\n",
        "    sns.pairplot(ticker_data, diag_kind=\"kde\", corner=True)  # `corner=True` removes duplicate plots\n",
        "    plt.suptitle(f\"Pairplot for {ticker.upper()}\", y=1.02)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLlFah1-1YH2"
      },
      "source": [
        "- Tech stocks (AAPL, MSFT, GOOG) react to trading volume and exhibit momentum-driven behavior.\n",
        "- Defensive stocks (JNJ, MCD, PEP) show minimal impact from short-term trading, making them stable investments.\n",
        "- Visa and Home Depot contribute steadily to portfolio performance with low volatility.\n",
        "-  Google is highly reactive to trading activity, making it attractive for traders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqlr1o4NE9fX"
      },
      "source": [
        "## Hypothesis SF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7CPyZEP4Vdx"
      },
      "source": [
        "### Does Higher Volume_HD (x-axis) correlate with lower Close_JNJ (y-axis) and is correlated to Close_V (by color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XMwialWY4V4_",
        "outputId": "788447e8-dbd7-4037-9d50-81045714a6c4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Define relevant columns\n",
        "volume_hd = \"Volume_HD\"\n",
        "close_jnj = \"Close_JNJ\"\n",
        "close_v = \"Close_V\"\n",
        "\n",
        "# Drop missing values for clean analysis\n",
        "df_clean = df_sf[[volume_hd, close_jnj, close_v]].dropna()\n",
        "\n",
        "# Compute correlation coefficients\n",
        "corr_volume_close_jnj, _ = spearmanr(df_clean[volume_hd], df_clean[close_jnj])\n",
        "corr_volume_close_v, _ = spearmanr(df_clean[volume_hd], df_clean[close_v])\n",
        "corr_jnj_v, _ = spearmanr(df_clean[close_jnj], df_clean[close_v])\n",
        "\n",
        "# Print correlation results\n",
        "print(f\"Spearman correlation between {volume_hd} and {close_jnj}: {corr_volume_close_jnj:.4f}\")\n",
        "print(f\"Spearman correlation between {volume_hd} and {close_v}: {corr_volume_close_v:.4f}\")\n",
        "print(f\"Spearman correlation between {close_jnj} and {close_v}: {corr_jnj_v:.4f}\")\n",
        "\n",
        "# --- First Scatter Plot (Without Trend Line) ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(\n",
        "    df_clean[volume_hd],\n",
        "    df_clean[close_jnj],\n",
        "    c=df_clean[close_v],\n",
        "    cmap='coolwarm',\n",
        "    edgecolor='k',\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.colorbar(label='Close_V')\n",
        "plt.xlabel(\"Volume_HD\")\n",
        "plt.ylabel(\"Close_JNJ\")\n",
        "plt.title(\"Multivariate Scatter Plot: Volume_HD vs Close_JNJ (colored by Close_V)\")\n",
        "plt.show()\n",
        "\n",
        "# --- Second Scatter Plot (With Trend Line) ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(\n",
        "    df_clean[volume_hd],\n",
        "    df_clean[close_jnj],\n",
        "    c=df_clean[close_v],\n",
        "    cmap='coolwarm',\n",
        "    edgecolor='k',\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.colorbar(label='Close_V')\n",
        "\n",
        "# Add a regression trend line\n",
        "sns.regplot(\n",
        "    x=df_clean[volume_hd],\n",
        "    y=df_clean[close_jnj],\n",
        "    scatter=False,  # Hide seaborn scatter points\n",
        "    line_kws={\"color\": \"black\", \"linewidth\": 2, \"linestyle\": \"--\"}  # Trend line style\n",
        ")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Volume_HD\")\n",
        "plt.ylabel(\"Close_JNJ\")\n",
        "plt.title(\"Multivariate Scatter Plot with Trend Line: Volume_HD vs Close_JNJ (colored by Close_V)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVw6AOf4bru"
      },
      "source": [
        "1. Why These Three Stocks?\n",
        "\n",
        "  For this hypothesis, I selected Home Depot (HD), Johnson & Johnson (JNJ), and Visa (V) from Scott Franklin’s portfolio because they are blue-chip stocks that are part of the Dow Jones Industrial Average (DJIA). These companies represent leaders in their respective sectors:\n",
        "\n",
        "  - Home Depot (HD) – A dominant player in the home improvement retail industry.\n",
        "  - Johnson & Johnson (JNJ) – A global healthcare and pharmaceutical giant.\n",
        "  - Visa (V) – A leader in the digital payments sector.\n",
        "\n",
        "  The reason for choosing these stocks is to examine whether changes in trading volume for Home Depot (HD) impact Johnson & Johnson (JNJ)’s closing price and whether Visa’s (V) closing price correlates with this relationship. Since Dow Jones blue-chip stocks often move in response to broader market trends, investor sentiment, and macroeconomic factors, analyzing their interactions can provide insights into sector rotation and institutional trading behavior.\n",
        "\n",
        "2. Hypothesis:\n",
        "\n",
        "  We test whether higher Volume_HD (x-axis) correlates with lower Close_JNJ (y-axis) and whether Close_JNJ is related to Close_V (by color).\n",
        "\n",
        "3. Observations of Scatter Plots:\n",
        "\n",
        "  No strong visible pattern, but some downward movement in Close_JNJ as Volume_HD increases.\n",
        "\n",
        "  The correlation between Close_JNJ and Close_V is negative, contradicting expectations. And as we can see more red dots (higher price of Close_V are on the bottom of the scatterplot.\n",
        "\n",
        "4. Conclusion:\n",
        "\n",
        "  We reject the hypothesis in general.\n",
        "\n",
        "  However, we fail to reject that higher Volume_HD correlates with lower Close_JNJ, as a weak negative trend is observed.\n",
        "\n",
        "  However, Close_JNJ and Close_V are negatively correlated, which was unexpected.\n",
        "\n",
        "  The relationships suggest sector-specific movements rather than strong cross-stock dependency within this portfolio.\n",
        "\n",
        "  All in all this analysis suggests that while Home Depot's trading activity may have a minor influence on Johnson & Johnson’s stock price, the relationship between JNJ and Visa is not as expected, indicating that different factors may be driving their price movements independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YO-kwwfgVIG"
      },
      "source": [
        "# Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "71loWR8YgZHO",
        "outputId": "0391988b-4455-48ff-c8db-3fd2050e2457"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the index to 'Date' if not already\n",
        "df_sf = df_sf.sort_index()\n",
        "\n",
        "# Plot each stock's closing price\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for col in df_sf.columns:\n",
        "    if \"Close_\" in col:  # Filter only closing price columns\n",
        "        plt.plot(df_sf.index, df_sf[col], label=col.replace(\"Close_\", \"\"))\n",
        "\n",
        "# Formatting the plot\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Closing Price\")\n",
        "plt.title(\"Time Series of Stock Closing Prices in df_sf\")\n",
        "plt.legend(title=\"Stocks\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SPl9nOeQhzbD",
        "outputId": "a5d4e42a-bfb6-4ed1-e0c6-7e010c3eda0f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the index is set to Date and sorted\n",
        "df_sf = df_sf.sort_index()\n",
        "\n",
        "# Loop through each stock and plot separately\n",
        "for col in df_sf.columns:\n",
        "    if \"Close_\" in col:  # Filter only closing price columns\n",
        "        stock_name = col.replace(\"Close_\", \"\")  # Extract stock ticker\n",
        "\n",
        "        # Create a new figure for each stock\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(df_sf.index, df_sf[col], label=stock_name, color='blue')\n",
        "\n",
        "        # Formatting the plot\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Closing Price\")\n",
        "        plt.title(f\"Time Series of {stock_name}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTqPjKYwWewc"
      },
      "source": [
        "1. AAPL (Apple Inc.)\n",
        "- Trend: Apple has a strong upward trend with occasional pullbacks, suggesting that its stock price has generally risen over time, with minor dips.\n",
        "- Volatility: Moderate volatility, meaning the stock price shows some fluctuations but remains within a relatively predictable range.\n",
        "\n",
        "2. MSFT (Microsoft Corp.)\n",
        "- Trend: The trend for Microsoft is sideways with a mild uptrend, indicating that while the stock price doesn't have significant increases, there is a gradual rise over time.\n",
        "- Volatility: Low volatility, suggesting that Microsoft's stock price is relatively stable and less prone to large swings.\n",
        "\n",
        "3. JPM (JP Morgan)\n",
        "- -Trend: Highly volatile with large swings, meaning the price of JPM stock tends to move significantly up and down.\n",
        "- Volatility: High volatility, with noticeable fluctuations that can lead to sharp changes in the stock price over short periods.\n",
        "\n",
        "4. MCD (McDonald's Corp.)\n",
        "- Trend: McDonald's has a steady upward trend, indicating consistent growth over time.\n",
        "- Volatility: Low volatility, suggesting McDonald's stock price moves in a stable manner without large fluctuations.\n",
        "\n",
        "5. V (Visa Inc.)\n",
        "- -Trend: Visa's price shows a smooth upward trend with mild pullbacks, implying steady growth with only occasional dips.\n",
        "- Volatility: Low volatility, as Visa's price remains relatively stable with mild fluctuations.\n",
        "\n",
        "6. WMT (Walmart)\n",
        "- Trend: Walmart's stock shows a modest upward trend with some stagnation, meaning the price rises at a slower pace with some periods where the stock price doesn't grow significantly.\n",
        "- Volatility: Moderate volatility, reflecting a moderate amount of fluctuation in stock prices.\n",
        "\n",
        "7. JNJ (Johnson & Johnson)\n",
        "- Trend: Johnson & Johnson exhibits a steady upward trend, meaning the stock price has generally increased over time without significant downward movements.\n",
        "- Volatility: Low volatility, indicating relatively stable performance with few fluctuations.\n",
        "\n",
        "8. HD (Home Depot Inc.)\n",
        "- Trend: Home Depot's stock has a strong upward trend, especially during periods of housing booms, suggesting it performs well when the housing market is strong.\n",
        "- Volatility: Low to moderate volatility, reflecting that while there are some fluctuations, the overall price movement is stable compared to other highly volatile stocks.\n",
        "\n",
        "9. GOOG (Alphabet Inc.)\n",
        "- Trend: Alphabet's stock shows a consistent upward trend with occasional volatility, meaning the stock tends to rise over time but can experience noticeable fluctuations.\n",
        "- Volatility: Moderate volatility, indicating moderate price swings around the general upward trend.\n",
        "\n",
        "10. PEP (PepsiCo Inc.)\n",
        "- Trend: PepsiCo has a steady, gradual upward trend, suggesting consistent growth over time with no sudden changes.\n",
        "- Volatility: Low volatility, meaning the stock price moves smoothly without significant fluctuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hkXBdLM6qB-y",
        "outputId": "0e1d4230-0203-45f4-9cd8-baed1a3bc7e4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract tickers\n",
        "tickers = {col.split('_')[-1] for col in df_sf.columns if 'Close_' in col}\n",
        "\n",
        "for ticker in tickers:\n",
        "    buy_col = f\"Buy_{ticker}\"\n",
        "    sell_col = f\"Sell_{ticker}\"\n",
        "\n",
        "    if buy_col in df_sf.columns and sell_col in df_sf.columns:\n",
        "        plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "\n",
        "        plt.plot(df_sf.index, df_sf[buy_col], label='Buy', color='green')\n",
        "        plt.plot(df_sf.index, df_sf[sell_col], label='Sell', color='red')\n",
        "\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.title(f\"Buy/Sell Trends for {ticker}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Buy or Sell column not found for {ticker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeUjQAhO4B_c"
      },
      "source": [
        "For each stock, the Buy and Sell trends indicate investor sentiment and market reactions to various factors ( earnings reports, market movements, product launches). Stocks like MSFT, V, and AAPL show frequent buy signals, indicating investor confidence and accumulation in anticipation of growth. Defensive stocks like MCD and JNJ display less sell-off activity, reflecting their long-term hold appeal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZN81dPRbijh4",
        "outputId": "b2257ac3-5258-437e-ed0b-f116a43e9d24"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Ensure the index is set to Date and sorted\n",
        "df_sf = df_sf.sort_index()\n",
        "\n",
        "# Loop through each stock and perform seasonal decomposition\n",
        "for col in df_sf.columns:\n",
        "    if \"Close_\" in col:  # Filter only closing price columns\n",
        "        stock_name = col.replace(\"Close_\", \"\")  # Extract stock ticker\n",
        "\n",
        "        # Drop NaN values to avoid issues with decomposition\n",
        "        stock_data = df_sf[col].dropna()\n",
        "\n",
        "        # Ensure we have enough data points for decomposition\n",
        "        if len(stock_data) < 60:\n",
        "            print(f\"Skipping {stock_name} due to insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        # Perform seasonal decomposition\n",
        "        decomposition = seasonal_decompose(stock_data, model=\"additive\", period=30)  # Adjust period as needed\n",
        "\n",
        "        # Plot decomposition results\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.subplot(411)\n",
        "        plt.plot(stock_data, label=\"Original Data\", color=\"blue\")\n",
        "        plt.legend(loc=\"upper left\")\n",
        "\n",
        "        plt.subplot(412)\n",
        "        plt.plot(decomposition.trend, label=\"Trend\", color=\"green\")\n",
        "        plt.legend(loc=\"upper left\")\n",
        "\n",
        "        plt.subplot(413)\n",
        "        plt.plot(decomposition.seasonal, label=\"Seasonality\", color=\"orange\")\n",
        "        plt.legend(loc=\"upper left\")\n",
        "\n",
        "        plt.subplot(414)\n",
        "        plt.plot(decomposition.resid, label=\"Residuals (Irregular Component)\", color=\"red\")\n",
        "        plt.legend(loc=\"upper left\")\n",
        "\n",
        "        plt.suptitle(f\"Seasonal Decomposition of {stock_name}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0L4HdJF4ju8"
      },
      "source": [
        "1. AAPL (Apple Inc.)\n",
        "- Trend: Shows a general upward trend, consistent with Apple's long-term growth, especially from innovation and strong market position.\n",
        "- Seasonality: There’s a slight seasonal component, potentially reflecting product cycles (e.g., new iPhone releases).\n",
        "- Residuals: The residuals show random fluctuations, indicating relatively clean data with no significant outliers.\n",
        "- Insights: The overall trend suggests strong growth, but seasonal peaks may align with quarterly earnings reports or product launches.\n",
        "\n",
        "2. MSFT (Microsoft Corp.)\n",
        "- Trend: A steady upward trend with occasional pauses, indicating Microsoft's stable and consistent growth over time.\n",
        "- Seasonality: A moderate seasonal cycle could reflect product updates or earnings seasonality.\n",
        "- Residuals: The residuals are small, indicating that most of the variance in the data is explained by the trend and seasonal components.\n",
        "- Insights: The data suggests a relatively predictable stock with cyclical behavior tied to quarterly earnings or market events.\n",
        "\n",
        "3. TSLA (Tesla Inc.)\n",
        "- Trend: The trend shows a sharp upward growth but with more volatility, aligning with Tesla's history of strong price movements.\n",
        "- Seasonality: Some seasonality is visible, possibly tied to production cycles or product launches (like new car models or major events with Elon Musk).\n",
        "- Residuals: High residuals, indicating large volatility that isn’t fully explained by the trend or seasonal cycles.\n",
        "- Insights: Tesla’s stock is highly volatile, and the trend shows strong growth, but it’s often subject to large fluctuations unrelated to typical market cycles.\n",
        "\n",
        "4. MCD (McDonald's Corp.)\n",
        "- Trend: McDonald's shows a slow, stable upward trend, reflecting its reliable earnings and position as a defensive stock.\n",
        "- Seasonality: There’s a slight seasonal cycle, which could align with consumer trends (e.g., summer increases in fast food consumption or holiday promotions).\n",
        "- Residuals: Low residuals, indicating that the trend and seasonality explain most of the price movements.\n",
        "- Insights: McDonald's is a defensive stock with a steady upward trend, and its price movements seem to reflect seasonal consumer behavior rather than major shifts in its core business.\n",
        "\n",
        "5. V (Visa Inc.)\n",
        "- Trend: Visa has a clear upward trend, indicating its dominance in the digital payments space and growth over time.\n",
        "- Seasonality: Mild seasonal fluctuations, which may reflect annual cycles in consumer spending or specific financial events.\n",
        "- Residuals: Low residuals, reinforcing that Visa’s price is largely explained by its trend and seasonality.\n",
        "- Insights: Visa’s stock shows steady growth, with minor seasonal fluctuations potentially tied to consumer activity and payment cycles.\n",
        "\n",
        "6. WIT (Wipro Ltd.)\n",
        "- Trend: Wipro shows a flattening trend, with no significant upward movement, indicating that it might be in a consolidation phase.\n",
        "- Seasonality: Mild seasonality, potentially reflecting IT outsourcing cycles or quarterly project completions.\n",
        "- Residuals: Moderate residuals, indicating that there are factors affecting Wipro's stock price that are not captured by the seasonal and trend components.\n",
        "- Insights: Wipro's flat trend may indicate low investor interest or market saturation, with seasonal fluctuations suggesting some periodicity linked to project timelines or industry cycles.\n",
        "\n",
        "7. JNJ (Johnson & Johnson)\n",
        "- -Trend: A strong upward trend, reflecting JNJ’s long-term stability in healthcare and pharmaceuticals.\n",
        "- Seasonality: Limited seasonality, which could be related to pharmaceutical cycles (e.g., quarterly earnings or regulatory approvals).\n",
        "- Residuals: The residuals are very low, showing that JNJ’s price is largely explained by its underlying trend and consistent performance.\n",
        "- Insights: Johnson & Johnson exhibits steady growth over time, with minimal seasonal impact, indicating its status as a defensive, reliable stock.\n",
        "\n",
        "8. HD (Home Depot Inc.)\n",
        "- Trend: The trend shows a strong upward trajectory, consistent with the booming housing market and home improvement trends.\n",
        "- Seasonality: Clear seasonal patterns likely linked to peak home improvement seasons, such as spring and summer.\n",
        "Residuals: Low residuals, indicating that most of the price movement can be explained by the trend and seasonality.\n",
        "- Insights: Home Depot’s stock is closely tied to housing and home improvement trends, with strong seasonal demand peaks and consistent growth in periods of high consumer spending on home projects.\n",
        "\n",
        "9. GOOG (Alphabet Inc.)\n",
        "- Trend: Google’s stock shows a consistent upward trend, reflecting its dominance in search and ad revenue.\n",
        "- Seasonality: Minor seasonal components, possibly reflecting advertising cycles or quarterly earnings reports.\n",
        "- Residuals: Minimal residuals, indicating that the stock’s behavior is well-explained by its trend and seasonality.\n",
        "- Insights: Google’s stock is driven by consistent growth with minor seasonal fluctuations, aligning with its strong market presence and ad-based revenue model.\n",
        "\n",
        "10. PEP (PepsiCo Inc.)\n",
        "- Trend: PepsiCo’s stock shows a gradual upward trend, driven by its steady revenue from snacks, beverages, and consumer goods.\n",
        "- Seasonality: Moderate seasonal fluctuations, likely tied to consumer behavior during peak holidays or summer months.\n",
        "- Residuals: Low residuals, indicating that PepsiCo’s stock price is largely explained by the trend and seasonal factors.\n",
        "- Insights: PepsiCo’s stable upward trajectory and moderate seasonal peaks reflect its consistent market position in the beverage and snack industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgEZrZ-diUZx"
      },
      "source": [
        "## Modeling -- BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ6WCH8cQAVE"
      },
      "source": [
        "Calculating the Adj Close Average for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR6DCFo1iWvV",
        "outputId": "96940054-2658-4190-8994-09c9ea809415"
      },
      "outputs": [],
      "source": [
        "from pyomo.environ import *\n",
        "import pandas as pd\n",
        "\n",
        "# Identify all columns that contain \"Adj Close_\"\n",
        "adj_cols = [col for col in df_sf.columns if col.startswith(\"Adj Close\")]\n",
        "\n",
        "# Extract tickers (assuming format \"Adj Close_{ticker}\")\n",
        "tickers = [col.split(\"_\")[1] for col in adj_cols]\n",
        "\n",
        "# Loop through each ticker and calculate the average adjusted closing price\n",
        "avg_adj_close = {}  # Use a dictionary to store the results\n",
        "for ticker in tickers:\n",
        "    avg_adj_close[ticker] = df_sf[f\"Adj Close_{ticker}\"].mean()\n",
        "\n",
        "# Print the result\n",
        "print(\"Average Adj Closing Price per stock:\")\n",
        "for ticker, avg_price in avg_adj_close.items():\n",
        "    print(f\"{ticker}: {avg_price}\")\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "df_sf_returns = pd.DataFrame(list(avg_adj_close.items()), columns=[\"Ticker\", \"Average_Adj_Close\"])\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(\"\\ndf_sf_returns:\")\n",
        "print(df_sf_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2MWTaywP9P1"
      },
      "source": [
        "Creating the covaraince matrix from the Average of Adj Close for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNLQvdoqiaO4"
      },
      "outputs": [],
      "source": [
        "# Select closing prices for all relevant tickers:\n",
        "close_prices_all = df_sf[[f\"Adj Close_{ticker}\" for ticker in tickers]]\n",
        "\n",
        "# Calculate the covariance matrix:\n",
        "df_sf_cov = close_prices_all.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF63Y21AicHH"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D33eXGyvifCn"
      },
      "outputs": [],
      "source": [
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xEmA8-lihPD"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToF4h6UJB4LM"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JR91JNDinTD"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSVBee7uiqwA",
        "outputId": "198e12bd-a758-44b7-e414-4275dd4cce16"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iplVZixiuCa",
        "outputId": "f8bcb43f-d09f-4ec6-f14e-f9aaf40734fb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from pylab import *\n",
        "\n",
        "import shutil\n",
        "import sys\n",
        "import os.path\n",
        "\n",
        "if not shutil.which(\"pyomo\"):\n",
        "    !pip install -q pyomo\n",
        "    assert(shutil.which(\"pyomo\"))\n",
        "\n",
        "if not shutil.which(\"ipopt\"):\n",
        "    # here is the IPOPT zip file\n",
        "    !gdown 10XRvLZqrpSNiXVAN-pipU52BVRwoGcNQ\n",
        "    !unzip -o -q ipopt-linux64_dw\n",
        "    assert(shutil.which(\"ipopt\") or os.path.isfile(\"ipopt\"))\n",
        "\n",
        "from pyomo.environ import *\n",
        "\n",
        "SOLVER = 'ipopt'\n",
        "EXECUTABLE = '/content/ipopt'\n",
        "ipopt_executable = '/content/ipopt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1ENiZyRiu2O",
        "outputId": "416b5d0d-7368-42cf-ae32-cdaf8c2610bc"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5XoNvmt9jFqa",
        "outputId": "89e88e80-7e52-404c-a047-f30d08b19a97"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling BASELINE: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling BASELINE: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling BASELINE: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IOnEGCDYtz"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.05).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, MCD, are also included in the portfolio once somewhere below 0.01 but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - The maximum return for this baseline model is 200."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vKLO-Jsy9B"
      },
      "source": [
        "### Ranking of Optimal Stock Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEPhT8VMDYRx",
        "outputId": "22b1ead5-75ad-4487-c725-111c7dd638a8"
      },
      "outputs": [],
      "source": [
        "# Choose a specific risk limit (for example, the maximum risk limit in your analysis)\n",
        "selected_risk = param_analysis.index.max()\n",
        "optimal_allocations = param_analysis.loc[selected_risk]\n",
        "\n",
        "# Sort the allocations in ascending order\n",
        "sorted_allocations = optimal_allocations.sort_values()\n",
        "\n",
        "# Print each stock with its allocation percentage\n",
        "print(f\"Optimal Allocation (sorted ascending) for risk limit {selected_risk}:\")\n",
        "for stock, allocation in sorted_allocations.items():\n",
        "    print(f\"{stock}: {allocation*100:.2f}%\")\n",
        "\n",
        "total = round(sorted_allocations.sum(), 10)  # Round to 10 decimal places\n",
        "print(f\"Sum of allocations = {total:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_OB4tNrSfUT"
      },
      "source": [
        "This confirms that PEP and JNJ are dominating the Stock Allocations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39D7qdAW-0Hu"
      },
      "source": [
        "## Modeling w/o PEP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A9aMtRHDMp7"
      },
      "source": [
        "Our baseline model indicated that PEP was dominating over 30% of the portfolio. To explore alternatives, we’re removing PEP from the decision variables while keeping all other conditions the same, allowing us to see which stock becomes the next most dominant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLPLoJrp-1r5"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y16UsBvECadg"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaR1315w-7Qb"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic0HEKyB--JE",
        "outputId": "d6f96828-841f-4191-97d7-a8320b97751d"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnuJw5ur_Bjs",
        "outputId": "67c1c255-60e7-4c5d-b12f-77525a6e2ab2"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCmQab1W_CYz",
        "outputId": "6c1df9b3-aded-4a20-d458-36c74a4b14cf"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o PEP: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o PEP: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(3, 3),       # 3 rows, 3 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o PEP: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sZK_vBXHrHD",
        "outputId": "7a57a3b3-f1ed-47a3-e8c8-d97093e038f9"
      },
      "outputs": [],
      "source": [
        "# Choose a specific risk limit (for example, the maximum risk limit in your analysis)\n",
        "selected_risk = param_analysis.index.max()\n",
        "optimal_allocations = param_analysis.loc[selected_risk]\n",
        "\n",
        "# Sort the allocations in ascending order\n",
        "sorted_allocations = optimal_allocations.sort_values()\n",
        "\n",
        "# Print each stock with its allocation percentage\n",
        "print(f\"Optimal Allocation (sorted ascending) for risk limit {selected_risk}:\")\n",
        "for stock, allocation in sorted_allocations.items():\n",
        "    print(f\"{stock}: {allocation*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrB9dmMWGkBJ"
      },
      "source": [
        "After taking out PEP, the next most dominate stock is JNJ which is expected when you look at the BASELINE model. It was dominating 14% of the portfolio the 2nd most dominant sotck.\n",
        "  - We are also starting to see other stocks becoming more dominate in the Optimal Stock Allocation like JNJ (40% of the portfolio).\n",
        "  - Without JNJ, the maximum return has dropped down to 175."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rikx2fLM_o3H"
      },
      "source": [
        "## Modeling w/o JNJ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhDoBGz8H7pQ"
      },
      "source": [
        "After removing JNJ, MSFT emerged as the most dominant stock. To identify the next most influential asset, we are now excluding both PEP and JNJ from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uhS5RfU_rjf"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFkn04AnCclU"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 171*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KsFjPgP_uvF"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxvDHXZ3_wg7",
        "outputId": "741ec1bf-4fc5-49c0-d653-3b33a6ca0317"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.GOOG]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LgX_Ki7_yU0",
        "outputId": "53e42641-caac-461b-e2f7-f2d265d5adc3"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.GOOG()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DoWlrz27_1L4",
        "outputId": "1135453f-9c38-4738-d0c9-89ea67a0a0be"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o JNJ: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o JNJ: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o JNJ: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKdP7pm9IDCj"
      },
      "source": [
        "After taking out JNJ, the next most dominate stock is MSFT which is expected when you look at the BASELINE model. It was dominating 14% of the portfolio the 2nd most dominant stock.\n",
        "  - We are also starting to see other stocks becoming more dominate in the Optimal Stock Allocation like GOOG and MCD.\n",
        "  - Without JNJ, the maximum return has increased to 300% indicating that JNJ was limiting the portfolio's return potential by anchoring a significant portion of the allocation, and its removal allows the model to shift towards stocks with higher return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oLflNtk_ovp"
      },
      "source": [
        "## Modeling w/o MSFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLrDWI8_JOqF"
      },
      "source": [
        "After removing JNJ and PEP, MSFT emerged as the most dominant stock. To identify the next most influential asset, we are now excluding MSFT, JNJ, and MSFT from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEs4jCR9_795"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-elUBIHrCf7M"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 287*V + 75*WMT + 279*MCD + 171*GOOG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCnabHZH__tt"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.WMT + m.MCD + m.GOOG == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.WMT + m.MCD + m.GOOG == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtVQYw5IABSr",
        "outputId": "9cb342a8-0bb9-4393-92e6-6fa1964ee299"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.V, m.WMT, m.MCD, m.GOOG]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_GOOG']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52LEY_6JAE5D",
        "outputId": "299edea9-6cac-47d4-83e8-76fdd2da4919"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.V(), m.WMT(), m.MCD(), m.GOOG()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QlMFwe3GAH0c",
        "outputId": "83de2d21-87fd-410a-de00-347954ff8d2f"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'V', 'WMT', 'MCD', 'GOOG']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o MSFT: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o MSFT: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o MSFT: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raEmGIOdJnJ2"
      },
      "source": [
        "After taking out MSFT, the next most dominate stock is GOOG which is expected when you look at the BASELINE model, it is one of the dominating stocks.\n",
        "  -  We are also seeing that some other stocks like HD, AAPL, JPM, and WMT are not being incorporated which could be indicating that under the current model parameters, only a very limited set of stocks contributes meaningfully to optimizing the risk-return balance.\n",
        "  - Without MSFT, the maximum return has decreased significantly to 200% indicating that MSFT is a crucial driver of the portfolio's high return potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tv85WLZAIsV"
      },
      "source": [
        "## Modeling w/o GOOG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK64LTIjJ-AB"
      },
      "source": [
        "After removing JNJ, PEP, and MSFT, GOOG emerged as the most dominant stock. To identify the next most influential asset, we are now also going to exclude GOOG from the decision variables, allowing us to analyze the portfolio composition without these key components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9333V7iyALA9"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8lCRA2JCiNK"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 287*V + 75*WMT + 279*MCD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG8addyhAPpl"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.WMT + m.MCD == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.WMT + m.MCD == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4TF4-i1ARhI",
        "outputId": "9df991a5-9173-4a55-d1fe-511b7b340d21"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.V, m.WMT, m.MCD]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lbu57ANATg1",
        "outputId": "9a725373-511b-4213-f46d-b7573c176549"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.V(), m.WMT(), m.MCD()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BhE9VP_eAWap",
        "outputId": "ff5b25b2-f661-485f-afe5-70d85d02911b"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'V', 'WMT', 'MCD']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o GOOG: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o GOOG: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o GOOG: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgfluiHMGHo"
      },
      "source": [
        "After taking out GOOG, the next most dominate stock is WMT.\n",
        "  -  We are still seeing that some other stocks like HD, AAPL, V, and JPM are not being incorporated which could be indicating that under the current model parameters, only a very limited set of stocks contributes meaningfully to optimizing the risk-return balance.\n",
        "  - Without GOOG, the maximum return has decreased significantly to 7% indicating that GOOG is a crucial driver of the portfolio's high return potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9dcpgYgAZyA"
      },
      "source": [
        "## Modeling w/o WMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICYmm3TLKGAB"
      },
      "source": [
        "After removing JNJ, PEP, MSFT, and GOOG from the decision variables, WMT emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2M_YGPJAbY5"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cMipEXECkUN"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 287*V + 279*MCD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz4sLjW3Afph"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.MCD == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V + m.MCD == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7ASnKewAhfZ",
        "outputId": "da5ad4c1-0ed3-4712-ef91-6d5df0edb955"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.V, m.MCD]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_V', 'Adj Close_MCD']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tivhOU_zAlHd",
        "outputId": "1c835642-d17f-4616-dddd-fb8d44e94423"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.V(), m.MCD()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "obWjGGGmAl1l",
        "outputId": "1db3c6c1-a54a-4c5a-dbfc-3ef726e1b0a4"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'V', 'MCD']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o WMT: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o WMT: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o WMT: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opXhD8J7Mnn5"
      },
      "source": [
        "After taking out WMT, the next most dominate stock is MCD.\n",
        "  - We are still seeing that some stocks like JPM and HD are not being incorporated which could be indicating that under the current model parameters, only a very limited set of stocks contributes meaningfully to optimizing the risk-return balance.\n",
        "  - Stocks like V, AAPL, and MCD are starting to play a role in the portfolio's risk-return balance.\n",
        "  - Without WMT, the maximum return has decreased to 3.5% indicating that WMT s a key driver in enhancing the portfolio's return potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNMG5_ohApuV"
      },
      "source": [
        "## Modeling w/o MCD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8gjPmhYKTNu"
      },
      "source": [
        "After removing JNJ, GOOG, MSFT, PEP, and WMT from the decision variables, MCD emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i-Wh1N1AoLv"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InLghWaCmKs"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 287*V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2PTkjC8AucK"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.V == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjOQ1rN_AwRu",
        "outputId": "e4e6b2c0-d885-46bf-c2ea-1b2df1d6ca10"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.V]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_V']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DWNsbAJAyLC",
        "outputId": "2b20ab5e-0e07-4e60-a351-240c638b5f5e"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.V()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QccvekjZA1Jr",
        "outputId": "146ebd82-b5f0-4ab2-f971-68335bcd2205"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'V']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o MCD: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o MCD: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o MCD: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHQ5MM3ANPwy"
      },
      "source": [
        "After taking out MCD, the next most dominate stock is V.\n",
        "  - We are still seeing that some stocks like JPM and HD are not being incorporated which could be indicating that under the current model parameters, only a very limited set of stocks contributes meaningfully to optimizing the risk-return balance.\n",
        "  - Without MCD, the maximum return has decreased to 2.5% indicating that MCD was a driver in enhancing the portfolio's return potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uacPq2A4H-"
      },
      "source": [
        "## Modeling w/o V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbeB5zUaKlBI"
      },
      "source": [
        "After removing JNJ, PEP, MSFT, GOOG, WMT, and MCD from the decision variables, V emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gOLmWNOA64B"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz9oyWaeCoOU"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF8K1jlXA-Ls"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_WICX-UA_vb",
        "outputId": "2cbf065d-1c3d-43e8-f5a1-473bbeb8a970"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P06kdsuqBBYu",
        "outputId": "b358b485-43e8-43cc-893b-36b91892da5e"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gHjEGuapBDj_",
        "outputId": "7bed3ada-4f4c-4c89-bf5f-328b0bfba935"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o V: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o V: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o V: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM3KHFL-Nu8W"
      },
      "source": [
        "After taking out V, the next most dominate stock is AAPL.\n",
        "  - All the stocks, AAPL, JPM, and HD are being incorporated to the portfolio.\n",
        "  - Without V, the maximum return has decreased to 2.0% indicating that MCD was a driver in enhancing the portfolio's return potential and the return is going to decrease as we take out more and more variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5TQOZGBGUX"
      },
      "source": [
        "## Modeling w/o AAPL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQRWcCYiKtPj"
      },
      "source": [
        "After removing JNJ, PEP, MSFT, GOOG, WMT, V, and MCD from the decision variables, AAPL emerged as the most dominant stock. To uncover the next most influential stock, we are now excluding these key components so we can analyze the portfolio composition to see which stock becomes the new leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8b3x9JoBI2e"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI93Y9M_C7n1"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 373*HD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GktrB8DLBK97"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.HD == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.HD == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0szwXFifBOe4",
        "outputId": "e30e233f-5d99-48c8-fb89-d4e11800c162"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.HD]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_HD']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qvXsnFYBQzk",
        "outputId": "6f86e36a-7102-4112-d363-c009954683af"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.HD()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1v0KGjI6BRdZ",
        "outputId": "21aadb22-f649-430c-b278-7f182550ff19"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'HD']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o AAPL: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o AAPL: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 18))\n",
        "\n",
        "# Iterate through the axes using flatten()\n",
        "for ax in axes.flatten():  # Use flatten() to iterate over a 1D array\n",
        "    i = axes.flatten().tolist().index(ax) #Get index for column name\n",
        "    param_analysis.iloc[:, i].plot(ax=ax, legend=True)  # Plot each column on a separate subplot\n",
        "    ax.set_xlabel(\"Risk Level\")\n",
        "    ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o AAPL: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRoSTgh_OVVO"
      },
      "source": [
        "After taking out AAPL, the next most dominant stock is JPM.\n",
        "  - Without AAPL the maximum return has stayed the same at 2%.\n",
        "  - Additionally, the optimized allocations for JPM and HD are very close to each other, suggesting that both stocks are nearly equally effective in driving the portfolio's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF8VJuIPBUGZ"
      },
      "source": [
        "## Modeling w/o JPM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bj_Giw9K0ZA"
      },
      "source": [
        "After removing PEP, JNJ, MSFT, GOOG, WMT, V, MCD, and AAPL from the decision variables, JPM emerged as the most dominant stock. We are going to analyze how HD does by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyB3s_vNBX3K"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "\n",
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-kvAYl7C9Lb"
      },
      "source": [
        "MAX(RETURN = 373*HD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oei46re8BZk0"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.HD == 0.05)\n",
        "m.sum_proportion = Constraint(expr = m.HD == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15vCtlnwBaBO",
        "outputId": "dba52bed-bdc0-49de-cc7a-5c8367d43dad"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.HD]\n",
        "  tickers = ['Adj Close_HD']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.05\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSXDgCwuBcIn",
        "outputId": "b8203934-412d-4e65-d60c-cce25414b210"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.HD()]\n",
        "  returns[r] = m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ePvJbWp0Bd5Y",
        "outputId": "29ba1d59-9197-4108-8793-92ca6fe25dcd"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['HD']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling w/o JPM: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling w/o JPM: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 18))\n",
        "\n",
        "# Remove or comment out the for loop and directly plot on 'axes'\n",
        "param_analysis.iloc[:, 0].plot(ax=axes, legend=True)  # Plot the first (and only) column on the axes\n",
        "axes.set_xlabel(\"Risk Level\")  # Set x-axis label for the axes\n",
        "axes.set_ylabel(\"Optimal Allocation\")  # Set y-axis label for the axes\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling w/o JPM: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXA4yn9cOnY6"
      },
      "source": [
        "After removing all decision variables except for HD, the portfolio's maximum return collapses dramatically—from 200% to just 2.5%, indicating that without the key components, the portfolio loses nearly all of its return-generating power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx_XIx-MjIap"
      },
      "source": [
        "## Modeling 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M2BbCq5jKeD"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxR1LgFijMft"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZMADPtOCRuJ"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK8N88ZMjPRA"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.1)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMlLd-JZjRc5",
        "outputId": "b906806e-4f3e-439c-ae09-1ffda66f6f92"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.1\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.001)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_p9lqgOjV7l",
        "outputId": "5b419575-85a4-46a6-e2e4-06ba8cbf3767"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UmvHVZcbjZsV",
        "outputId": "3aa5f218-8cfc-49f0-bdf1-c81cfb4d1f18"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 10%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 10%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 10%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnBm_6BwDhUK"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.1).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, MCD, are also included in the portfolio once at risk level 0.0813 but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.0813 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Returns dropped from 200 to 175 after increasing the maximum risk to 10%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8hqeoqEjeBX"
      },
      "source": [
        "## Modeling 25%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aCaP7zFjgZr"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX_j-LREjiWX"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHwjl8oDCSkJ"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiIO3_AGjlf6"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.25)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uwQ44YPjonC",
        "outputId": "1275a074-49e3-4b65-8b0c-ad7b6faca671"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.25\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0025)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DtZJC3jjtcj",
        "outputId": "31a52626-ff1d-4f53-dbb6-fa5be58e15e8"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t8QjfU4Oj6LK",
        "outputId": "e8a4c476-3e3d-409f-a874-0e0482755273"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 25%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 25%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 25%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNZ8iwZDmWS"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.25).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, MCD, are also included in the portfolio once around risk level 0.2 and 0.25 but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - Returns increase back to 200 after increasing the maximum risk to 25%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxJJkNq7j74m"
      },
      "source": [
        "## Modeling 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXDOGdl2j9fH"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnstMZ6kALD"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgFOluPDCTZl"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omEaicjrkEFX"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.5)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knah-lCikK50",
        "outputId": "a0223752-a797-424c-d08f-9eb5072c83bb"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.5\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.005)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7qIUHUzkR_n",
        "outputId": "0ba39f83-4fb3-49ee-bace-40cd190f2910"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JNwo83kBkYPn",
        "outputId": "371fd46f-5637-4e27-c67d-c512e338c4e8"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 50%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 50%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 50%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZlD80QvDrFz"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.5).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, and MCD, are not included in the portfolio at all.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqrR3H5mkbaE"
      },
      "source": [
        "## Modeling 75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfNZCIvqkc6E"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7JhzUwMkfuz"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWkPe8MGCUGl"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9xBlLY-kjJo"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.75)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5fi38lgk1ZV",
        "outputId": "54b7263a-d018-4f99-89b1-4326e745c2cc"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.75\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0075)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmR_tMezlM-C",
        "outputId": "cca20d46-f3ce-4ac1-fba8-462f199086a9"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iWBfTSallRSn",
        "outputId": "fed7fe97-da7e-40d5-ae78-dc8990da8365"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 75%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 75%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 75%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly1_V_vNDtLR"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.75).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, MCD, are also included in the portfolio at risk levels 0.1053 and 0.2328 but at a miniscule proportion.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - At specific risk levels, a tiny increase in allowed risk allows the model to pick a very different mix of stocks that can result in significantly higher returns.\n",
        "    - For example, specifically at risk levels of 0.1053 the model produces an unusal mix of stocks (all 10 stocks) and allocations, confirming that a critical threshold is reached within the optimization process.\n",
        "  - Returns increase from 200 to 250 after increasing the maximum risk to 75%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-sKkgflS3g"
      },
      "source": [
        "## Modeling 95%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkSVPpJolUSq"
      },
      "outputs": [],
      "source": [
        "m = ConcreteModel()\n",
        "# Decision Variables\n",
        "m.JPM = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.AAPL = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.HD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MSFT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.V = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.WMT = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.MCD = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.JNJ = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.GOOG = Var(within=NonNegativeReals, bounds= (0,1))\n",
        "m.PEP = Var(within=NonNegativeReals, bounds= (0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiYWpOGclWRd"
      },
      "outputs": [],
      "source": [
        "m.Objective = Objective(expr = df_sf_returns[\"Average_Adj_Close\"].iloc[0] * m.JPM +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[1] * m.AAPL +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[2] * m.HD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[3] * m.MSFT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[4] * m.V +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[5] * m.WMT +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[6] * m.MCD +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[7] * m.JNJ +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[8] * m.GOOG +\n",
        "                        df_sf_returns[\"Average_Adj_Close\"].iloc[9] * m.PEP, sense = maximize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL-c8Ak8CU0T"
      },
      "source": [
        "MAX(RETURN = 215*JPM + 213*AAPL + 373*HD + 422*MSFT + 287*V + 75*WMT + 279*MCD + 152*JNJ + 171*GOOG + 163*PEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCpZg7UlYdh"
      },
      "outputs": [],
      "source": [
        "m.total_risk = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 0.95)\n",
        "m.sum_proportion = Constraint(expr = m.JPM + m.AAPL + m.HD + m.MSFT + m.V + m.WMT + m.MCD + m.JNJ + m.GOOG + m.PEP == 1)\n",
        "m.return_floor = Constraint(expr = m.Objective >= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NObyxa3claOj",
        "outputId": "aa10fec4-f1dc-403c-956e-3f4ea4bf5345"
      },
      "outputs": [],
      "source": [
        "def calc_risk(m):\n",
        "  variables = [m.JPM, m.AAPL, m.HD, m.MSFT, m.V, m.WMT, m.MCD, m.JNJ, m.GOOG, m.PEP]\n",
        "  tickers = ['Adj Close_JPM', 'Adj Close_AAPL', 'Adj Close_HD', 'Adj Close_MSFT', 'Adj Close_V', 'Adj Close_WMT', 'Adj Close_MCD', 'Adj Close_JNJ', 'Adj Close_GOOG', 'Adj Close_PEP']\n",
        "  risk_exp = 0\n",
        "\n",
        "  for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "      risk_exp += variables[i] * df_sf_cov.at[tickers[i], tickers[j]] * variables[j]\n",
        "  return risk_exp\n",
        "\n",
        "expr_risk = calc_risk(m)\n",
        "\n",
        "max_risk = 0.95\n",
        "\n",
        "import numpy as np\n",
        "risk_limits = np.arange(0.0003, max_risk, 0.0095)  # take tiny steps\n",
        "risk_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oq6qVvElmxO",
        "outputId": "779da66d-3e25-4f72-aa98-ed79d5a95f63"
      },
      "outputs": [],
      "source": [
        "param_analysis = {} #paramater analysis --- risk vs return\n",
        "returns = {} #{} dict --\n",
        "for r in risk_limits:\n",
        "  m.del_component(m.total_risk)\n",
        "  m.total_risk = Constraint(expr = expr_risk <= r)\n",
        "  result = SolverFactory('ipopt', executable=ipopt_executable).solve(m).write()\n",
        "  param_analysis[r] = [m.JPM(), m.AAPL(), m.HD(), m.MSFT(), m.V(), m.WMT(), m.MCD(), m.JNJ(), m.GOOG(), m.PEP()]\n",
        "  returns[r] =  m.JPM()*df_sf_returns[\"Average_Adj_Close\"].iloc[0] + m.AAPL()*df_sf_returns[\"Average_Adj_Close\"].iloc[1] + m.HD()*df_sf_returns[\"Average_Adj_Close\"].iloc[2] + m.MSFT()*df_sf_returns[\"Average_Adj_Close\"].iloc[3] + m.V()*df_sf_returns[\"Average_Adj_Close\"].iloc[4] + m.WMT()*df_sf_returns[\"Average_Adj_Close\"].iloc[5] + m.MCD()*df_sf_returns[\"Average_Adj_Close\"].iloc[6] + m.JNJ()*df_sf_returns[\"Average_Adj_Close\"].iloc[7] + m.GOOG()*df_sf_returns[\"Average_Adj_Close\"].iloc[8] + m.PEP()*df_sf_returns[\"Average_Adj_Close\"].iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rDEGCAH0lrrA",
        "outputId": "2f58a59e-4fa3-4e85-ff5d-fb5f5a2a8047"
      },
      "outputs": [],
      "source": [
        "# Generate proportion of the portfolio for each risk limit\n",
        "param_analysis = pd.DataFrame.from_dict(param_analysis, orient='index')\n",
        "param_analysis.columns = [['JPM', 'AAPL', 'HD', 'MSFT', 'V', 'WMT', 'MCD', 'JNJ', 'GOOG', 'PEP']]\n",
        "param_analysis.index.name = 'Risk Limit'\n",
        "param_analysis.plot(figsize=(10, 6))\n",
        "plt.title('Modeling 95%: Optimal Stock Allocation for Diffrent Risk Levels')\n",
        "plt.xlabel('Risk Label')\n",
        "plt.ylabel('Optimal Stock Allocation')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar chart of the portfolio for each risk limit\n",
        "plt.figure(figsize=(30, 20))\n",
        "param_analysis.plot(kind='bar', stacked=True, ax=plt.gca(), width=1.0)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Modeling 95%: Stacked Bar Chart of Optimal Stock Allocation for Different Risk Levels\", fontsize=14)\n",
        "plt.xlabel(\"Risk Level\", fontsize=12)\n",
        "plt.ylabel(\"Allocation\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Stocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Seperate graphs for each stock\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 18))\n",
        "param_analysis.plot(\n",
        "    subplots=True,\n",
        "    layout=(5, 2),       # 5 rows, 2 columns\n",
        "    ax=axes,             # use our custom axes\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "for row in axes:\n",
        "    for ax in row:\n",
        "        ax.set_xlabel(\"Risk Level\")\n",
        "        ax.set_ylabel(\"Optimal Allocation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print risk and reward\n",
        "risk = list(returns.keys())\n",
        "print(f\"Risk\", risk)\n",
        "reward = list(returns.values())\n",
        "print(f\"Reward\", reward)\n",
        "print('\\t')\n",
        "\n",
        "from pylab import *\n",
        "# Plot risk and reward\n",
        "plt.figure(figsize=(10,6))\n",
        "plot(risk, reward, '-.')\n",
        "title('Modeling 95%: The Efficient Frontier')\n",
        "xlabel('Risk')\n",
        "ylabel('Reward (Return)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D-TEOLND2EP"
      },
      "source": [
        "The Optimal Stock Allocation for Different Risk Levels charts (stacked bar graph for better visibility) is showing how an \"optimal\" stock allocation (y-axis) breaks down across different risk levels (x-axis).\n",
        "  - In this case the risk levels ranges from (0.0003, 0.95).\n",
        "\n",
        "- Two stocks, PEP and JNJ, dominates all of the bars on the stacked bar graph.\n",
        "  - This indicates that the model often allocates PEP and JNJ in higher proportions and is always included in the portfolio.\n",
        "- Stocks like GOOG, V, and MSFT are also often included in the portfolio but at a lower proportion than JNJ.\n",
        "- JPM, AAPL, HD, WMT, and MCD, are not included in the portfolio at all.\n",
        "\n",
        "The Efficient Frontier shows how returns change with different levels of risk.\n",
        "  - The non-linear optimization reveals thresholds where a marginal change in risk constraints triggers a substantial rebalancing of asset allocations, yielding higher expected returns while also increasing volatility.\n",
        "  - There are small changes in risk which are causing big increases in the portfolio's return.\n",
        "    - This can be due to the portfolio being highly sensitive to minor changes in risk parameters.\n",
        "  - Returns drop back to 200 after increasing the maximum risk to 95%.\n",
        "  - Overall, while higher risk generally leads to higher returns, the relationship is not linear.\n",
        "    - The chart illustrates that returns increase with risk, but there are distinct thresholds where even a slight increase in risk yields a disproportionately large increase in return, resulting in an efficient frontier with sudden jumps rather than a gradual increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6hosr8ZPgbw"
      },
      "source": [
        "## Scott's Model Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mXCBXMvsLDr"
      },
      "source": [
        "-  Across different risk levels, a few stocks—specifically PEP and JNJ—consistently dominate the portfolio. Their presence is pivotal in achieving high returns, while other stocks only contribute at specific risk thresholds.\n",
        "\n",
        "- The Efficient Frontier reveals that even marginal changes in risk constraints can trigger significant rebalancing. For example, at a risk level of 0.1053, the model produces an unusual mix of stocks, underscoring that small adjustments can lead to disproportionately large increases in return.\n",
        "\n",
        "- Although higher risk levels generally lead to higher returns, the increase is not linear. While our baseline model achieves 200% return at a 5% risk level, higher risk models (up to 95%) do not consistently improve the risk–return balance; in some cases, returns plateau or even decline due to increased volatility and concentration risk.\n",
        "\n",
        "Overall, these models highlight that while expanding risk tolerance allows for broader stock allocations and potentially higher returns, it also introduces instability. The efficiency of the portfolio depends on striking the right balance, as even slight changes in risk constraints can have dramatic impacts on return potential."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
